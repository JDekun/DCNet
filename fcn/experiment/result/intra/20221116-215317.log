WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
| distributed init (rank 1): env://
| distributed init (rank 0): env://
Namespace(data_path='../../../input/pascal', device='cuda', num_classes=20, batch_size=12, batch_size_val=8, aux=True, start_epoch=20, epochs=100, sync_bn=False, workers=4, lr=0.0001, momentum=0.9, weight_decay=0.0001, print_freq=5, checkpoint_dir='./checkpoint/intra/20221116-215317', resume='checkpoint/intra/20221116-194523/model_19.pth', test_only=False, world_size=2, dist_url='env://', amp=True, seed=304, name_date='intra/20221116-215317', wandb=True, wandb_model='run', model_name='dcnet_resnet50', project_dim=128, loss_name='intra', contrast=True, rank=0, gpu=0, distributed=True, dist_backend='nccl')
Creating data loaders
Creating model
missing_keys:  ['classifier.L3u.0.weight', 'classifier.L3u.1.weight', 'classifier.L3u.1.bias', 'classifier.L3u.1.running_mean', 'classifier.L3u.1.running_var', 'classifier.L2u.0.weight', 'classifier.L2u.1.weight', 'classifier.L2u.1.bias', 'classifier.L2u.1.running_mean', 'classifier.L2u.1.running_var', 'classifier.L1u.0.weight', 'classifier.L1u.1.weight', 'classifier.L1u.1.bias', 'classifier.L1u.1.running_mean', 'classifier.L1u.1.running_var', 'classifier.cls.1.weight', 'classifier.cls.1.bias', 'ProjectorHead_3d.0.weight', 'ProjectorHead_3d.1.weight', 'ProjectorHead_3d.1.bias', 'ProjectorHead_3d.1.running_mean', 'ProjectorHead_3d.1.running_var', 'ProjectorHead_3d.3.weight', 'ProjectorHead_3d.4.weight', 'ProjectorHead_3d.4.bias', 'ProjectorHead_3d.4.running_mean', 'ProjectorHead_3d.4.running_var']
unexpected_keys:  ['classifier.0.weight', 'classifier.1.weight', 'classifier.1.bias', 'classifier.1.running_mean', 'classifier.1.running_var', 'classifier.1.num_batches_tracked', 'classifier.4.weight', 'classifier.4.bias']
Traceback (most recent call last):
  File "/home/zk/GIT/working/DCNet/fcn/train_multi_GPU.py", line 379, in <module>
    main(args)
  File "/home/zk/GIT/working/DCNet/fcn/train_multi_GPU.py", line 181, in main
    missing_keys, unexpected_keys = checkpoint = torch.load(args.resume, map_location='cpu')  # 读取之前保存的权重文件(包括优化器以及学习率策略)
ValueError: too many values to unpack (expected 2)
Traceback (most recent call last):
  File "/home/zk/GIT/working/DCNet/fcn/train_multi_GPU.py", line 379, in <module>
    main(args)
  File "/home/zk/GIT/working/DCNet/fcn/train_multi_GPU.py", line 181, in main
    missing_keys, unexpected_keys = checkpoint = torch.load(args.resume, map_location='cpu')  # 读取之前保存的权重文件(包括优化器以及学习率策略)
ValueError: too many values to unpack (expected 2)
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 6615) of binary: /home/zk/anaconda3/bin/python
Traceback (most recent call last):
  File "/home/zk/anaconda3/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/zk/anaconda3/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/zk/anaconda3/lib/python3.9/site-packages/torch/distributed/run.py", line 761, in main
    run(args)
  File "/home/zk/anaconda3/lib/python3.9/site-packages/torch/distributed/run.py", line 752, in run
    elastic_launch(
  File "/home/zk/anaconda3/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/zk/anaconda3/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_multi_GPU.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2022-11-16_21:53:35
  host      : GPU1
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 6616)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2022-11-16_21:53:35
  host      : GPU1
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 6615)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
