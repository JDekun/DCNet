{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:torch.distributed.run:\n",
            "*****************************************\n",
            "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
            "*****************************************\n",
            "| distributed init (rank 1): env://\n",
            "| distributed init (rank 2): env://\n",
            "| distributed init (rank 0): env://\n",
            "| distributed init (rank 3): env://\n",
            "debug/2023-0401-112224\n",
            "Namespace(data_path='pascal-voc-2012', device='cuda', num_classes=21, batch_size=8, batch_size_val=6, aux=False, start_epoch=0, epochs=20, sync_bn=False, workers=4, lr=0.0001, momentum=0.9, weight_decay=0.0001, print_freq=10, checkpoint_dir='./results/debug/2023-0401-112224', resume='', test_only=False, world_size=4, dist_url='env://', amp=True, seed=304, name_date='debug/2023-0401-112224', wandb=False, wandb_model='dryrun', run_name='', model_name='aspp_contrast_resnet101', project_dim=128, loss_name='aspp_loss', contrast=0, pre_trained='deeplabv3_resnet101_coco.pth', L3_loss=0.1, L2_loss=0.1, L1_loss=0.1, GAcc=1, memory_size=0, proj_dim=128, network_stride=8, pixel_update_freq=10, ddp=False, weight_only_backbone=False, sample='self_pace_epochs', rank=0, gpu=0, distributed=True, dist_backend='nccl')\n",
            "Creating data loaders\n",
            "Creating model\n",
            "missing_keys:  ['contrast.convs.0.0.weight', 'contrast.convs.0.1.weight', 'contrast.convs.0.1.bias', 'contrast.convs.0.1.running_mean', 'contrast.convs.0.1.running_var', 'contrast.convs.0.3.weight', 'contrast.convs.0.4.weight', 'contrast.convs.0.4.bias', 'contrast.convs.0.4.running_mean', 'contrast.convs.0.4.running_var', 'contrast.convs.1.0.weight', 'contrast.convs.1.1.weight', 'contrast.convs.1.1.bias', 'contrast.convs.1.1.running_mean', 'contrast.convs.1.1.running_var', 'contrast.convs.1.3.weight', 'contrast.convs.1.4.weight', 'contrast.convs.1.4.bias', 'contrast.convs.1.4.running_mean', 'contrast.convs.1.4.running_var', 'contrast.convs.2.0.weight', 'contrast.convs.2.1.weight', 'contrast.convs.2.1.bias', 'contrast.convs.2.1.running_mean', 'contrast.convs.2.1.running_var', 'contrast.convs.2.3.weight', 'contrast.convs.2.4.weight', 'contrast.convs.2.4.bias', 'contrast.convs.2.4.running_mean', 'contrast.convs.2.4.running_var']\n",
            "unexpected_keys:  ['aux_classifier.0.weight', 'aux_classifier.1.weight', 'aux_classifier.1.bias', 'aux_classifier.1.running_mean', 'aux_classifier.1.running_var', 'aux_classifier.1.num_batches_tracked', 'aux_classifier.4.weight', 'aux_classifier.4.bias']\n",
            "DistributedDataParallel(\n",
            "  (module): DeepLabV3(\n",
            "    (backbone): IntermediateLayerGetter(\n",
            "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "      (layer1): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (layer2): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (3): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (layer3): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (3): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (4): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (5): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (6): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (7): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (8): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (9): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (10): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (11): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (12): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (13): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (14): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (15): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (16): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (17): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (18): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (19): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (20): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (21): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (22): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (layer4): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
            "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
            "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
            "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (classifier): DeepLabHead(\n",
            "      (0): ASPP(\n",
            "        (convs): ModuleList(\n",
            "          (0): Sequential(\n",
            "            (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (1): ASPPConv(\n",
            "            (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), bias=False)\n",
            "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (2): ASPPConv(\n",
            "            (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(24, 24), dilation=(24, 24), bias=False)\n",
            "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (3): ASPPConv(\n",
            "            (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(36, 36), dilation=(36, 36), bias=False)\n",
            "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): ReLU(inplace=True)\n",
            "          )\n",
            "          (4): ASPPPooling(\n",
            "            (0): AdaptiveAvgPool2d(output_size=1)\n",
            "            (1): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (3): ReLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "        (project): Sequential(\n",
            "          (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Dropout(p=0.5, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (3): ReLU(inplace=True)\n",
            "      (4): Conv2d(256, 21, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (contrast): contrast_head(\n",
            "      (convs): ModuleList(\n",
            "        (0): ASPPContrast(\n",
            "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (5): ReLU(inplace=True)\n",
            "        )\n",
            "        (1): ASPPContrast(\n",
            "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (5): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): ASPPContrast(\n",
            "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (5): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Start training\n",
            "Epoch: [0] Train  [  0/330]  eta: 1:15:38  lr: 0.000000  loss: 1.7138 (1.7138)  time: 13.7544  data: 0.7810  max mem: 7782\n",
            "Epoch: [0] Train  [ 10/330]  eta: 0:57:44  lr: 0.000003  loss: 1.8666 (1.9833)  time: 10.8269  data: 0.0713  max mem: 8262\n",
            "Epoch: [0] Train  [ 20/330]  eta: 0:54:54  lr: 0.000006  loss: 1.6096 (2.0039)  time: 10.4701  data: 0.0002  max mem: 8262\n",
            "Epoch: [0] Train  [ 30/330]  eta: 0:52:56  lr: 0.000009  loss: 2.4270 (1.9716)  time: 10.4578  data: 0.0002  max mem: 8262\n",
            "Epoch: [0] Train  [ 40/330]  eta: 0:50:58  lr: 0.000013  loss: 2.1704 (1.9357)  time: 10.4642  data: 0.0002  max mem: 8262\n",
            "Epoch: [0] Train  [ 50/330]  eta: 0:49:10  lr: 0.000016  loss: 2.5054 (1.9207)  time: 10.4603  data: 0.0002  max mem: 8262\n",
            "Epoch: [0] Train  [ 60/330]  eta: 0:47:28  lr: 0.000019  loss: 1.9997 (1.9197)  time: 10.5501  data: 0.0002  max mem: 8262\n",
            "Epoch: [0] Train  [ 70/330]  eta: 0:45:40  lr: 0.000022  loss: 1.8617 (1.9271)  time: 10.5411  data: 0.0002  max mem: 8262\n",
            "Epoch: [0] Train  [ 80/330]  eta: 0:43:52  lr: 0.000025  loss: 1.5675 (1.9111)  time: 10.4730  data: 0.0002  max mem: 8262\n",
            "Epoch: [0] Train  [ 90/330]  eta: 0:42:05  lr: 0.000028  loss: 2.1084 (1.9486)  time: 10.4689  data: 0.0002  max mem: 8262\n",
            "Epoch: [0] Train  [100/330]  eta: 0:40:17  lr: 0.000031  loss: 1.3242 (1.9238)  time: 10.4451  data: 0.0002  max mem: 8262\n",
            "Epoch: [0] Train  [110/330]  eta: 0:38:31  lr: 0.000034  loss: 1.6404 (1.9083)  time: 10.4261  data: 0.0002  max mem: 8262\n",
            "Epoch: [0] Train  [120/330]  eta: 0:36:45  lr: 0.000037  loss: 1.2073 (1.8773)  time: 10.4485  data: 0.0002  max mem: 8262\n",
            "Epoch: [0] Train  [130/330]  eta: 0:34:59  lr: 0.000040  loss: 1.4277 (1.8543)  time: 10.4657  data: 0.0002  max mem: 8262\n",
            "Epoch: [0] Train  [140/330]  eta: 0:33:14  lr: 0.000043  loss: 1.0217 (1.8242)  time: 10.4591  data: 0.0002  max mem: 8262\n",
            "Epoch: [0] Train  [150/330]  eta: 0:31:28  lr: 0.000046  loss: 1.5871 (1.7911)  time: 10.4522  data: 0.0002  max mem: 8262\n",
            "Epoch: [0] Train  [160/330]  eta: 0:29:43  lr: 0.000049  loss: 1.6928 (1.7788)  time: 10.4598  data: 0.0002  max mem: 8262\n",
            "Epoch: [0] Train  [170/330]  eta: 0:27:58  lr: 0.000052  loss: 1.0033 (1.7598)  time: 10.4670  data: 0.0002  max mem: 8262\n",
            "Epoch: [0] Train  [180/330]  eta: 0:26:14  lr: 0.000055  loss: 0.8805 (1.7216)  time: 10.5071  data: 0.0002  max mem: 8262\n",
            "Epoch: [0] Train  [190/330]  eta: 0:24:28  lr: 0.000058  loss: 1.0784 (1.6935)  time: 10.4869  data: 0.0002  max mem: 8262\n",
            "Epoch: [0] Train  [200/330]  eta: 0:22:43  lr: 0.000061  loss: 0.7448 (1.6669)  time: 10.4366  data: 0.0003  max mem: 8262\n",
            "Epoch: [0] Train  [210/330]  eta: 0:20:58  lr: 0.000064  loss: 1.1183 (1.6372)  time: 10.4421  data: 0.0003  max mem: 8262\n",
            "Epoch: [0] Train  [220/330]  eta: 0:19:12  lr: 0.000067  loss: 0.9256 (1.6059)  time: 10.4196  data: 0.0002  max mem: 8262\n",
            "Epoch: [0] Train  [230/330]  eta: 0:17:28  lr: 0.000070  loss: 1.2385 (1.5821)  time: 10.4293  data: 0.0002  max mem: 8262\n",
            "Epoch: [0] Train  [240/330]  eta: 0:15:43  lr: 0.000073  loss: 0.7482 (1.5633)  time: 10.4872  data: 0.0002  max mem: 8262\n",
            "Epoch: [0] Train  [250/330]  eta: 0:13:58  lr: 0.000076  loss: 0.8030 (1.5395)  time: 10.4563  data: 0.0002  max mem: 8262\n",
            "Epoch: [0] Train  [260/330]  eta: 0:12:13  lr: 0.000079  loss: 1.0407 (1.5111)  time: 10.4192  data: 0.0002  max mem: 8262\n",
            "Epoch: [0] Train  [270/330]  eta: 0:10:28  lr: 0.000082  loss: 1.6506 (1.5020)  time: 10.4342  data: 0.0002  max mem: 8262\n",
            "Epoch: [0] Train  [280/330]  eta: 0:08:43  lr: 0.000085  loss: 0.5111 (1.4689)  time: 10.4187  data: 0.0002  max mem: 8262\n",
            "Epoch: [0] Train  [290/330]  eta: 0:06:58  lr: 0.000088  loss: 0.7197 (1.4471)  time: 10.4350  data: 0.0002  max mem: 8262\n",
            "Epoch: [0] Train  [300/330]  eta: 0:05:14  lr: 0.000091  loss: 0.3954 (1.4204)  time: 10.4562  data: 0.0002  max mem: 8262\n",
            "Epoch: [0] Train  [310/330]  eta: 0:03:29  lr: 0.000094  loss: 0.5676 (1.4043)  time: 10.4458  data: 0.0002  max mem: 8262\n",
            "Epoch: [0] Train  [320/330]  eta: 0:01:44  lr: 0.000097  loss: 0.6563 (1.3856)  time: 10.4585  data: 0.0002  max mem: 8262\n",
            "Epoch: [0] Train Total time: 0:57:35\n",
            "Epoch: [0] Test  [ 0/61]  eta: 0:02:32    time: 2.4989  data: 0.7048  max mem: 8262\n",
            "Epoch: [0] Test  [10/61]  eta: 0:00:57    time: 1.1282  data: 0.0643  max mem: 8262\n",
            "Epoch: [0] Test  [20/61]  eta: 0:00:46    time: 1.0761  data: 0.0002  max mem: 8262\n",
            "Epoch: [0] Test  [30/61]  eta: 0:00:34    time: 1.1184  data: 0.0003  max mem: 8262\n",
            "Epoch: [0] Test  [40/61]  eta: 0:00:22    time: 1.0076  data: 0.0003  max mem: 8262\n",
            "Epoch: [0] Test  [50/61]  eta: 0:00:12    time: 1.0501  data: 0.0003  max mem: 8262\n",
            "Epoch: [0] Test  [60/61]  eta: 0:00:01    time: 1.0639  data: 0.0002  max mem: 8262\n",
            "Epoch: [0] Test Total time: 0:01:05\n",
            "global correct: 95.3\n",
            "average row correct: ['97.5', '95.7', '86.0', '92.1', '86.0', '76.1', '98.2', '83.3', '95.9', '65.8', '96.1', '55.8', '88.6', '92.3', '94.9', '96.7', '70.0', '94.8', '79.4', '95.9', '82.6']\n",
            "IoU: ['94.6', '90.5', '42.7', '89.1', '72.5', '70.3', '97.0', '77.6', '91.9', '51.1', '92.1', '51.0', '86.5', '87.7', '90.5', '90.3', '62.6', '90.2', '60.7', '90.8', '80.4']\n",
            "mean IoU: 79.1\n",
            "Epoch: [1] Train  [  0/330]  eta: 0:59:07  lr: 0.000100  loss: 0.9354 (0.9354)  time: 10.7501  data: 0.5318  max mem: 8262\n",
            "Epoch: [1] Train  [ 10/330]  eta: 0:56:08  lr: 0.000100  loss: 0.4930 (0.8699)  time: 10.5263  data: 0.0486  max mem: 8262\n",
            "Epoch: [1] Train  [ 20/330]  eta: 0:54:24  lr: 0.000100  loss: 0.9148 (0.8914)  time: 10.5191  data: 0.0002  max mem: 8262\n",
            "Epoch: [1] Train  [ 30/330]  eta: 0:52:27  lr: 0.000100  loss: 0.5079 (0.8540)  time: 10.4748  data: 0.0002  max mem: 8262\n",
            "Epoch: [1] Train  [ 40/330]  eta: 0:50:32  lr: 0.000099  loss: 0.4848 (0.8317)  time: 10.3793  data: 0.0003  max mem: 8262\n",
            "Epoch: [1] Train  [ 50/330]  eta: 0:48:44  lr: 0.000099  loss: 0.5878 (0.7814)  time: 10.3732  data: 0.0003  max mem: 8262\n",
            "Epoch: [1] Train  [ 60/330]  eta: 0:46:59  lr: 0.000099  loss: 0.6372 (0.7662)  time: 10.4145  data: 0.0003  max mem: 8262\n",
            "Epoch: [1] Train  [ 70/330]  eta: 0:45:14  lr: 0.000099  loss: 0.7954 (0.7720)  time: 10.4191  data: 0.0003  max mem: 8262\n",
            "Epoch: [1] Train  [ 80/330]  eta: 0:43:30  lr: 0.000099  loss: 0.3473 (0.7375)  time: 10.4362  data: 0.0003  max mem: 8262\n",
            "Epoch: [1] Train  [ 90/330]  eta: 0:41:45  lr: 0.000099  loss: 0.1683 (0.7125)  time: 10.4528  data: 0.0003  max mem: 8262\n",
            "Epoch: [1] Train  [100/330]  eta: 0:40:02  lr: 0.000099  loss: 0.6420 (0.6971)  time: 10.4701  data: 0.0003  max mem: 8262\n",
            "Epoch: [1] Train  [110/330]  eta: 0:38:20  lr: 0.000098  loss: 1.0072 (0.7072)  time: 10.5193  data: 0.0003  max mem: 8262\n",
            "Epoch: [1] Train  [120/330]  eta: 0:36:35  lr: 0.000098  loss: 0.2532 (0.7012)  time: 10.4897  data: 0.0003  max mem: 8262\n",
            "Epoch: [1] Train  [130/330]  eta: 0:34:51  lr: 0.000098  loss: 0.5932 (0.6861)  time: 10.4802  data: 0.0003  max mem: 8262\n",
            "Epoch: [1] Train  [140/330]  eta: 0:33:06  lr: 0.000098  loss: 0.1857 (0.6958)  time: 10.4817  data: 0.0003  max mem: 8262\n",
            "Epoch: [1] Train  [150/330]  eta: 0:31:22  lr: 0.000098  loss: 0.5522 (0.6919)  time: 10.4537  data: 0.0004  max mem: 8262\n",
            "Epoch: [1] Train  [160/330]  eta: 0:29:37  lr: 0.000098  loss: 1.0079 (0.7046)  time: 10.4468  data: 0.0004  max mem: 8262\n",
            "Epoch: [1] Train  [170/330]  eta: 0:27:52  lr: 0.000098  loss: 0.4621 (0.6953)  time: 10.3929  data: 0.0004  max mem: 8262\n",
            "Epoch: [1] Train  [180/330]  eta: 0:26:08  lr: 0.000097  loss: 0.4907 (0.6885)  time: 10.4305  data: 0.0004  max mem: 8262\n",
            "Epoch: [1] Train  [190/330]  eta: 0:24:23  lr: 0.000097  loss: 0.6138 (0.6981)  time: 10.4830  data: 0.0003  max mem: 8262\n",
            "Epoch: [1] Train  [200/330]  eta: 0:22:39  lr: 0.000097  loss: 1.6775 (0.6916)  time: 10.4881  data: 0.0002  max mem: 8262\n",
            "Epoch: [1] Train  [210/330]  eta: 0:20:54  lr: 0.000097  loss: 0.2302 (0.6977)  time: 10.4596  data: 0.0002  max mem: 8262\n",
            "Epoch: [1] Train  [220/330]  eta: 0:19:10  lr: 0.000097  loss: 0.6594 (0.6914)  time: 10.4448  data: 0.0003  max mem: 8262\n",
            "Epoch: [1] Train  [230/330]  eta: 0:17:25  lr: 0.000097  loss: 1.1417 (0.6790)  time: 10.4581  data: 0.0003  max mem: 8262\n",
            "Epoch: [1] Train  [240/330]  eta: 0:15:40  lr: 0.000097  loss: 2.7816 (0.6827)  time: 10.4213  data: 0.0004  max mem: 8262\n",
            "Epoch: [1] Train  [250/330]  eta: 0:13:56  lr: 0.000096  loss: 0.2956 (0.6834)  time: 10.4441  data: 0.0003  max mem: 8262\n",
            "Epoch: [1] Train  [260/330]  eta: 0:12:11  lr: 0.000096  loss: 0.3853 (0.6845)  time: 10.4843  data: 0.0002  max mem: 8262\n",
            "Epoch: [1] Train  [270/330]  eta: 0:10:27  lr: 0.000096  loss: 1.0168 (0.6951)  time: 10.4918  data: 0.0003  max mem: 8262\n",
            "Epoch: [1] Train  [280/330]  eta: 0:08:42  lr: 0.000096  loss: 0.5707 (0.6969)  time: 10.4633  data: 0.0003  max mem: 8262\n",
            "Epoch: [1] Train  [290/330]  eta: 0:06:58  lr: 0.000096  loss: 0.3067 (0.6998)  time: 10.4546  data: 0.0003  max mem: 8262\n",
            "Epoch: [1] Train  [300/330]  eta: 0:05:13  lr: 0.000096  loss: 0.3913 (0.6974)  time: 10.5046  data: 0.0003  max mem: 8262\n",
            "Epoch: [1] Train  [310/330]  eta: 0:03:29  lr: 0.000096  loss: 1.0202 (0.6935)  time: 10.4815  data: 0.0003  max mem: 8262\n",
            "Epoch: [1] Train  [320/330]  eta: 0:01:44  lr: 0.000095  loss: 0.7373 (0.6910)  time: 10.4302  data: 0.0003  max mem: 8262\n",
            "Epoch: [1] Train Total time: 0:57:31\n",
            "Epoch: [1] Test  [ 0/61]  eta: 0:01:46    time: 1.7512  data: 0.6936  max mem: 8262\n",
            "Epoch: [1] Test  [10/61]  eta: 0:00:52    time: 1.0379  data: 0.0633  max mem: 8262\n",
            "Epoch: [1] Test  [20/61]  eta: 0:00:44    time: 1.0620  data: 0.0002  max mem: 8262\n",
            "Epoch: [1] Test  [30/61]  eta: 0:00:33    time: 1.1171  data: 0.0002  max mem: 8262\n",
            "Epoch: [1] Test  [40/61]  eta: 0:00:22    time: 1.0045  data: 0.0002  max mem: 8262\n",
            "Epoch: [1] Test  [50/61]  eta: 0:00:11    time: 1.0372  data: 0.0002  max mem: 8262\n",
            "Epoch: [1] Test  [60/61]  eta: 0:00:01    time: 1.0550  data: 0.0002  max mem: 8262\n",
            "Epoch: [1] Test Total time: 0:01:04\n",
            "global correct: 95.4\n",
            "average row correct: ['97.0', '97.4', '88.6', '94.7', '85.6', '86.9', '98.1', '92.1', '97.1', '70.8', '94.9', '61.4', '90.6', '93.9', '95.3', '95.2', '73.7', '95.0', '83.3', '95.6', '89.5']\n",
            "IoU: ['94.7', '88.5', '43.0', '89.6', '72.3', '76.6', '96.4', '81.1', '91.7', '48.7', '91.5', '55.7', '86.5', '88.7', '91.0', '90.5', '64.0', '89.4', '62.9', '91.2', '79.5']\n",
            "mean IoU: 79.7\n",
            "Epoch: [2] Train  [  0/330]  eta: 0:58:17  lr: 0.000095  loss: 0.5080 (0.5080)  time: 10.5999  data: 0.5088  max mem: 8262\n",
            "Epoch: [2] Train  [ 10/330]  eta: 0:55:58  lr: 0.000095  loss: 1.1666 (0.6832)  time: 10.4958  data: 0.0465  max mem: 8262\n",
            "Epoch: [2] Train  [ 20/330]  eta: 0:54:12  lr: 0.000095  loss: 2.6012 (0.7419)  time: 10.4860  data: 0.0002  max mem: 8262\n",
            "Epoch: [2] Train  [ 30/330]  eta: 0:52:23  lr: 0.000095  loss: 1.3331 (0.6920)  time: 10.4697  data: 0.0002  max mem: 8262\n",
            "Epoch: [2] Train  [ 40/330]  eta: 0:50:38  lr: 0.000095  loss: 0.3379 (0.6460)  time: 10.4641  data: 0.0002  max mem: 8262\n",
            "Epoch: [2] Train  [ 50/330]  eta: 0:48:50  lr: 0.000095  loss: 0.3218 (0.6360)  time: 10.4499  data: 0.0002  max mem: 8262\n",
            "Epoch: [2] Train  [ 60/330]  eta: 0:47:06  lr: 0.000094  loss: 0.3503 (0.6383)  time: 10.4472  data: 0.0002  max mem: 8262\n",
            "Epoch: [2] Train  [ 70/330]  eta: 0:45:22  lr: 0.000094  loss: 0.5796 (0.6299)  time: 10.4768  data: 0.0003  max mem: 8262\n",
            "Epoch: [2] Train  [ 80/330]  eta: 0:43:38  lr: 0.000094  loss: 0.8596 (0.6332)  time: 10.4872  data: 0.0002  max mem: 8262\n",
            "Epoch: [2] Train  [ 90/330]  eta: 0:41:54  lr: 0.000094  loss: 1.1270 (0.6336)  time: 10.4971  data: 0.0002  max mem: 8262\n",
            "Epoch: [2] Train  [100/330]  eta: 0:40:11  lr: 0.000094  loss: 0.2540 (0.6329)  time: 10.5234  data: 0.0003  max mem: 8262\n",
            "Epoch: [2] Train  [110/330]  eta: 0:38:25  lr: 0.000094  loss: 0.4451 (0.6410)  time: 10.4952  data: 0.0003  max mem: 8262\n",
            "Epoch: [2] Train  [120/330]  eta: 0:36:40  lr: 0.000094  loss: 0.5195 (0.6261)  time: 10.4562  data: 0.0003  max mem: 8262\n",
            "Epoch: [2] Train  [130/330]  eta: 0:34:55  lr: 0.000093  loss: 0.1708 (0.6222)  time: 10.4501  data: 0.0003  max mem: 8262\n",
            "Epoch: [2] Train  [140/330]  eta: 0:33:10  lr: 0.000093  loss: 0.5505 (0.6106)  time: 10.4562  data: 0.0003  max mem: 8262\n",
            "Epoch: [2] Train  [150/330]  eta: 0:31:25  lr: 0.000093  loss: 0.7724 (0.6185)  time: 10.4828  data: 0.0002  max mem: 8262\n",
            "Epoch: [2] Train  [160/330]  eta: 0:29:40  lr: 0.000093  loss: 0.2727 (0.6058)  time: 10.4852  data: 0.0002  max mem: 8262\n",
            "Epoch: [2] Train  [170/330]  eta: 0:27:55  lr: 0.000093  loss: 0.3810 (0.6094)  time: 10.4651  data: 0.0002  max mem: 8262\n",
            "Epoch: [2] Train  [180/330]  eta: 0:26:11  lr: 0.000093  loss: 0.5126 (0.6018)  time: 10.4544  data: 0.0002  max mem: 8262\n",
            "Epoch: [2] Train  [190/330]  eta: 0:24:25  lr: 0.000092  loss: 0.4951 (0.6073)  time: 10.4341  data: 0.0002  max mem: 8262\n",
            "Epoch: [2] Train  [200/330]  eta: 0:22:41  lr: 0.000092  loss: 0.2610 (0.6126)  time: 10.4582  data: 0.0002  max mem: 8262\n",
            "Epoch: [2] Train  [210/330]  eta: 0:20:56  lr: 0.000092  loss: 0.2637 (0.6157)  time: 10.4897  data: 0.0002  max mem: 8262\n",
            "Epoch: [2] Train  [220/330]  eta: 0:19:12  lr: 0.000092  loss: 0.5745 (0.6138)  time: 10.4923  data: 0.0002  max mem: 8262\n",
            "Epoch: [2] Train  [230/330]  eta: 0:17:27  lr: 0.000092  loss: 0.1715 (0.6077)  time: 10.4869  data: 0.0002  max mem: 8262\n",
            "Epoch: [2] Train  [240/330]  eta: 0:15:42  lr: 0.000092  loss: 0.8143 (0.6036)  time: 10.4630  data: 0.0002  max mem: 8262\n",
            "Epoch: [2] Train  [250/330]  eta: 0:13:57  lr: 0.000092  loss: 0.4091 (0.6062)  time: 10.4468  data: 0.0002  max mem: 8262\n",
            "Epoch: [2] Train  [260/330]  eta: 0:12:13  lr: 0.000091  loss: 0.5671 (0.6129)  time: 10.4632  data: 0.0002  max mem: 8262\n",
            "Epoch: [2] Train  [270/330]  eta: 0:10:28  lr: 0.000091  loss: 0.6724 (0.6099)  time: 10.4611  data: 0.0003  max mem: 8262\n",
            "Epoch: [2] Train  [280/330]  eta: 0:08:43  lr: 0.000091  loss: 0.1655 (0.6008)  time: 10.4654  data: 0.0003  max mem: 8262\n",
            "Epoch: [2] Train  [290/330]  eta: 0:06:58  lr: 0.000091  loss: 0.7090 (0.6066)  time: 10.4780  data: 0.0002  max mem: 8262\n",
            "Epoch: [2] Train  [300/330]  eta: 0:05:14  lr: 0.000091  loss: 0.4336 (0.6009)  time: 10.4545  data: 0.0003  max mem: 8262\n",
            "Epoch: [2] Train  [310/330]  eta: 0:03:29  lr: 0.000091  loss: 0.4112 (0.5988)  time: 10.4606  data: 0.0003  max mem: 8262\n",
            "Epoch: [2] Train  [320/330]  eta: 0:01:44  lr: 0.000091  loss: 0.1840 (0.5974)  time: 10.4886  data: 0.0003  max mem: 8262\n",
            "Epoch: [2] Train Total time: 0:57:35\n",
            "Epoch: [2] Test  [ 0/61]  eta: 0:01:45    time: 1.7355  data: 0.6591  max mem: 8262\n",
            "Epoch: [2] Test  [10/61]  eta: 0:00:52    time: 1.0382  data: 0.0602  max mem: 8262\n",
            "Epoch: [2] Test  [20/61]  eta: 0:00:44    time: 1.0622  data: 0.0003  max mem: 8262\n",
            "Epoch: [2] Test  [30/61]  eta: 0:00:33    time: 1.1151  data: 0.0003  max mem: 8262\n",
            "Epoch: [2] Test  [40/61]  eta: 0:00:22    time: 1.0021  data: 0.0002  max mem: 8262\n",
            "Epoch: [2] Test  [50/61]  eta: 0:00:11    time: 1.0368  data: 0.0002  max mem: 8262\n",
            "Epoch: [2] Test  [60/61]  eta: 0:00:01    time: 1.0560  data: 0.0002  max mem: 8262\n",
            "Epoch: [2] Test Total time: 0:01:04\n",
            "global correct: 95.6\n",
            "average row correct: ['96.9', '97.0', '88.9', '93.5', '87.1', '89.2', '98.2', '89.5', '97.3', '68.0', '94.2', '64.9', '96.0', '94.0', '95.7', '95.5', '81.3', '95.3', '82.6', '97.2', '88.6']\n",
            "IoU: ['94.8', '91.1', '43.1', '89.4', '69.4', '77.1', '96.7', '81.7', '93.7', '48.3', '91.2', '57.1', '89.2', '91.1', '91.3', '90.8', '66.4', '88.5', '63.8', '91.2', '80.1']\n",
            "mean IoU: 80.3\n",
            "Epoch: [3] Train  [  0/330]  eta: 0:58:46  lr: 0.000090  loss: 0.4778 (0.4778)  time: 10.6850  data: 0.5288  max mem: 8262\n",
            "Epoch: [3] Train  [ 10/330]  eta: 0:56:04  lr: 0.000090  loss: 0.2757 (0.5838)  time: 10.5152  data: 0.0483  max mem: 8262\n",
            "Epoch: [3] Train  [ 20/330]  eta: 0:54:13  lr: 0.000090  loss: 0.5871 (0.5786)  time: 10.4845  data: 0.0002  max mem: 8262\n",
            "Epoch: [3] Train  [ 30/330]  eta: 0:52:24  lr: 0.000090  loss: 0.8146 (0.6060)  time: 10.4646  data: 0.0002  max mem: 8262\n",
            "Epoch: [3] Train  [ 40/330]  eta: 0:50:41  lr: 0.000090  loss: 0.3789 (0.5824)  time: 10.4788  data: 0.0002  max mem: 8262\n",
            "Epoch: [3] Train  [ 50/330]  eta: 0:48:54  lr: 0.000090  loss: 0.3946 (0.5528)  time: 10.4812  data: 0.0003  max mem: 8262\n",
            "Epoch: [3] Train  [ 60/330]  eta: 0:47:11  lr: 0.000090  loss: 0.5023 (0.5600)  time: 10.4839  data: 0.0002  max mem: 8262\n",
            "Epoch: [3] Train  [ 70/330]  eta: 0:45:27  lr: 0.000089  loss: 0.3678 (0.5749)  time: 10.5063  data: 0.0002  max mem: 8262\n",
            "Epoch: [3] Train  [ 80/330]  eta: 0:43:43  lr: 0.000089  loss: 2.1968 (0.5837)  time: 10.5184  data: 0.0002  max mem: 8262\n",
            "Epoch: [3] Train  [ 90/330]  eta: 0:41:59  lr: 0.000089  loss: 0.3329 (0.5869)  time: 10.5363  data: 0.0002  max mem: 8262\n",
            "Epoch: [3] Train  [100/330]  eta: 0:40:13  lr: 0.000089  loss: 0.5808 (0.5913)  time: 10.4950  data: 0.0002  max mem: 8262\n",
            "Epoch: [3] Train  [110/330]  eta: 0:38:28  lr: 0.000089  loss: 1.0744 (0.6003)  time: 10.4645  data: 0.0002  max mem: 8262\n",
            "Epoch: [3] Train  [120/330]  eta: 0:36:43  lr: 0.000089  loss: 0.5237 (0.5972)  time: 10.4992  data: 0.0002  max mem: 8262\n",
            "Epoch: [3] Train  [130/330]  eta: 0:34:59  lr: 0.000089  loss: 0.3415 (0.5809)  time: 10.5068  data: 0.0002  max mem: 8262\n",
            "Epoch: [3] Train  [140/330]  eta: 0:33:14  lr: 0.000088  loss: 0.1545 (0.5737)  time: 10.5204  data: 0.0002  max mem: 8262\n",
            "Epoch: [3] Train  [150/330]  eta: 0:31:29  lr: 0.000088  loss: 0.3368 (0.5646)  time: 10.4925  data: 0.0002  max mem: 8262\n",
            "Epoch: [3] Train  [160/330]  eta: 0:29:44  lr: 0.000088  loss: 0.2245 (0.5761)  time: 10.4641  data: 0.0002  max mem: 8262\n",
            "Epoch: [3] Train  [170/330]  eta: 0:27:58  lr: 0.000088  loss: 0.7752 (0.5717)  time: 10.4585  data: 0.0002  max mem: 8262\n",
            "Epoch: [3] Train  [180/330]  eta: 0:26:13  lr: 0.000088  loss: 0.3780 (0.5652)  time: 10.4517  data: 0.0002  max mem: 8262\n",
            "Epoch: [3] Train  [190/330]  eta: 0:24:28  lr: 0.000088  loss: 0.2332 (0.5622)  time: 10.4519  data: 0.0002  max mem: 8262\n",
            "Epoch: [3] Train  [200/330]  eta: 0:22:43  lr: 0.000088  loss: 0.2301 (0.5542)  time: 10.4689  data: 0.0002  max mem: 8262\n",
            "Epoch: [3] Train  [210/330]  eta: 0:20:58  lr: 0.000087  loss: 0.6726 (0.5533)  time: 10.5007  data: 0.0003  max mem: 8262\n",
            "Epoch: [3] Train  [220/330]  eta: 0:19:13  lr: 0.000087  loss: 0.6566 (0.5531)  time: 10.4903  data: 0.0003  max mem: 8262\n",
            "Epoch: [3] Train  [230/330]  eta: 0:17:28  lr: 0.000087  loss: 0.3775 (0.5501)  time: 10.4655  data: 0.0002  max mem: 8262\n",
            "Epoch: [3] Train  [240/330]  eta: 0:15:43  lr: 0.000087  loss: 0.5889 (0.5548)  time: 10.4795  data: 0.0002  max mem: 8262\n",
            "Epoch: [3] Train  [250/330]  eta: 0:13:59  lr: 0.000087  loss: 0.3718 (0.5486)  time: 10.5370  data: 0.0002  max mem: 8262\n",
            "Epoch: [3] Train  [260/330]  eta: 0:12:14  lr: 0.000087  loss: 0.4829 (0.5440)  time: 10.5499  data: 0.0002  max mem: 8262\n",
            "Epoch: [3] Train  [270/330]  eta: 0:10:29  lr: 0.000087  loss: 0.4124 (0.5403)  time: 10.5225  data: 0.0002  max mem: 8262\n",
            "Epoch: [3] Train  [280/330]  eta: 0:08:44  lr: 0.000086  loss: 0.1671 (0.5381)  time: 10.5126  data: 0.0002  max mem: 8262\n",
            "Epoch: [3] Train  [290/330]  eta: 0:06:59  lr: 0.000086  loss: 0.5053 (0.5367)  time: 10.5058  data: 0.0002  max mem: 8262\n",
            "Epoch: [3] Train  [300/330]  eta: 0:05:14  lr: 0.000086  loss: 1.2198 (0.5373)  time: 10.5048  data: 0.0002  max mem: 8262\n",
            "Epoch: [3] Train  [310/330]  eta: 0:03:29  lr: 0.000086  loss: 0.3827 (0.5337)  time: 10.5188  data: 0.0002  max mem: 8262\n",
            "Epoch: [3] Train  [320/330]  eta: 0:01:44  lr: 0.000086  loss: 0.2861 (0.5326)  time: 10.4895  data: 0.0002  max mem: 8262\n",
            "Epoch: [3] Train Total time: 0:57:42\n",
            "Epoch: [3] Test  [ 0/61]  eta: 0:01:56    time: 1.9037  data: 0.8230  max mem: 8262\n",
            "Epoch: [3] Test  [10/61]  eta: 0:00:53    time: 1.0542  data: 0.0751  max mem: 8262\n",
            "Epoch: [3] Test  [20/61]  eta: 0:00:45    time: 1.0617  data: 0.0003  max mem: 8262\n",
            "Epoch: [3] Test  [30/61]  eta: 0:00:33    time: 1.1141  data: 0.0003  max mem: 8262\n",
            "Epoch: [3] Test  [40/61]  eta: 0:00:22    time: 1.0018  data: 0.0003  max mem: 8262\n",
            "Epoch: [3] Test  [50/61]  eta: 0:00:11    time: 1.0379  data: 0.0003  max mem: 8262\n",
            "Epoch: [3] Test  [60/61]  eta: 0:00:01    time: 1.0575  data: 0.0003  max mem: 8262\n",
            "Epoch: [3] Test Total time: 0:01:04\n",
            "global correct: 95.4\n",
            "average row correct: ['96.6', '98.2', '93.3', '96.9', '88.6', '90.9', '98.4', '92.9', '97.9', '66.9', '95.2', '62.5', '96.2', '94.6', '96.5', '95.2', '83.8', '96.1', '81.5', '98.0', '90.2']\n",
            "IoU: ['94.7', '90.2', '42.4', '85.9', '67.4', '77.0', '96.8', '82.9', '92.6', '49.0', '91.5', '56.4', '88.7', '91.0', '91.4', '90.7', '65.2', '85.6', '65.0', '91.3', '79.8']\n",
            "mean IoU: 79.8\n",
            "Epoch: [4] Train  [  0/330]  eta: 0:59:31  lr: 0.000086  loss: 0.3888 (0.3888)  time: 10.8215  data: 0.5503  max mem: 8262\n",
            "Epoch: [4] Train  [ 10/330]  eta: 0:56:18  lr: 0.000086  loss: 0.3377 (0.8269)  time: 10.5588  data: 0.0503  max mem: 8262\n",
            "Epoch: [4] Train  [ 20/330]  eta: 0:54:29  lr: 0.000085  loss: 0.1699 (0.5833)  time: 10.5320  data: 0.0002  max mem: 8262\n",
            "Epoch: [4] Train  [ 30/330]  eta: 0:52:36  lr: 0.000085  loss: 0.3628 (0.5467)  time: 10.5003  data: 0.0003  max mem: 8262\n",
            "Epoch: [4] Train  [ 40/330]  eta: 0:50:48  lr: 0.000085  loss: 0.4321 (0.5703)  time: 10.4781  data: 0.0003  max mem: 8262\n",
            "Epoch: [4] Train  [ 50/330]  eta: 0:48:59  lr: 0.000085  loss: 0.2918 (0.5787)  time: 10.4652  data: 0.0003  max mem: 8262\n",
            "Epoch: [4] Train  [ 60/330]  eta: 0:47:11  lr: 0.000085  loss: 0.7643 (0.5944)  time: 10.4288  data: 0.0003  max mem: 8262\n",
            "Epoch: [4] Train  [ 70/330]  eta: 0:45:27  lr: 0.000085  loss: 0.2684 (0.5650)  time: 10.4697  data: 0.0002  max mem: 8262\n",
            "Epoch: [4] Train  [ 80/330]  eta: 0:43:37  lr: 0.000084  loss: 0.6866 (0.5535)  time: 10.4290  data: 0.0002  max mem: 8262\n",
            "Epoch: [4] Train  [ 90/330]  eta: 0:41:52  lr: 0.000084  loss: 0.6017 (0.5678)  time: 10.3975  data: 0.0002  max mem: 8262\n",
            "Epoch: [4] Train  [100/330]  eta: 0:40:09  lr: 0.000084  loss: 0.2445 (0.5663)  time: 10.4925  data: 0.0002  max mem: 8262\n",
            "Epoch: [4] Train  [110/330]  eta: 0:38:25  lr: 0.000084  loss: 0.5025 (0.5610)  time: 10.5272  data: 0.0002  max mem: 8262\n",
            "Epoch: [4] Train  [120/330]  eta: 0:36:40  lr: 0.000084  loss: 1.5984 (0.5605)  time: 10.5017  data: 0.0002  max mem: 8262\n",
            "Epoch: [4] Train  [130/330]  eta: 0:34:55  lr: 0.000084  loss: 0.5351 (0.5534)  time: 10.4510  data: 0.0003  max mem: 8262\n",
            "Epoch: [4] Train  [140/330]  eta: 0:33:10  lr: 0.000084  loss: 0.3358 (0.5377)  time: 10.4504  data: 0.0003  max mem: 8262\n",
            "Epoch: [4] Train  [150/330]  eta: 0:31:25  lr: 0.000083  loss: 0.6983 (0.5326)  time: 10.4563  data: 0.0002  max mem: 8262\n",
            "Epoch: [4] Train  [160/330]  eta: 0:29:40  lr: 0.000083  loss: 0.2940 (0.5288)  time: 10.4702  data: 0.0002  max mem: 8262\n",
            "Epoch: [4] Train  [170/330]  eta: 0:27:56  lr: 0.000083  loss: 0.5168 (0.5407)  time: 10.5107  data: 0.0002  max mem: 8262\n",
            "Epoch: [4] Train  [180/330]  eta: 0:26:11  lr: 0.000083  loss: 0.1197 (0.5365)  time: 10.4953  data: 0.0002  max mem: 8262\n",
            "Epoch: [4] Train  [190/330]  eta: 0:24:27  lr: 0.000083  loss: 0.8262 (0.5380)  time: 10.4933  data: 0.0003  max mem: 8262\n",
            "Epoch: [4] Train  [200/330]  eta: 0:22:41  lr: 0.000083  loss: 0.4774 (0.5341)  time: 10.4662  data: 0.0002  max mem: 8262\n",
            "Epoch: [4] Train  [210/330]  eta: 0:20:57  lr: 0.000083  loss: 0.3347 (0.5312)  time: 10.4775  data: 0.0002  max mem: 8262\n",
            "Epoch: [4] Train  [220/330]  eta: 0:19:13  lr: 0.000082  loss: 0.2632 (0.5257)  time: 10.5422  data: 0.0002  max mem: 8262\n",
            "Epoch: [4] Train  [230/330]  eta: 0:17:28  lr: 0.000082  loss: 0.1510 (0.5214)  time: 10.4997  data: 0.0003  max mem: 8262\n",
            "Epoch: [4] Train  [240/330]  eta: 0:15:43  lr: 0.000082  loss: 0.2790 (0.5112)  time: 10.4424  data: 0.0003  max mem: 8262\n",
            "Epoch: [4] Train  [250/330]  eta: 0:13:58  lr: 0.000082  loss: 0.3299 (0.5068)  time: 10.4454  data: 0.0003  max mem: 8262\n",
            "Epoch: [4] Train  [260/330]  eta: 0:12:13  lr: 0.000082  loss: 0.5223 (0.5000)  time: 10.4730  data: 0.0003  max mem: 8262\n",
            "Epoch: [4] Train  [270/330]  eta: 0:10:28  lr: 0.000082  loss: 0.3363 (0.4991)  time: 10.4911  data: 0.0002  max mem: 8262\n",
            "Epoch: [4] Train  [280/330]  eta: 0:08:43  lr: 0.000082  loss: 0.2862 (0.4949)  time: 10.4715  data: 0.0002  max mem: 8262\n",
            "Epoch: [4] Train  [290/330]  eta: 0:06:59  lr: 0.000081  loss: 0.4213 (0.4967)  time: 10.4355  data: 0.0002  max mem: 8262\n",
            "Epoch: [4] Train  [300/330]  eta: 0:05:14  lr: 0.000081  loss: 0.2684 (0.4944)  time: 10.4903  data: 0.0002  max mem: 8262\n",
            "Epoch: [4] Train  [310/330]  eta: 0:03:29  lr: 0.000081  loss: 0.4941 (0.4896)  time: 10.4957  data: 0.0002  max mem: 8262\n",
            "Epoch: [4] Train  [320/330]  eta: 0:01:44  lr: 0.000081  loss: 0.5508 (0.4867)  time: 10.4126  data: 0.0003  max mem: 8262\n",
            "Epoch: [4] Train Total time: 0:57:35\n",
            "Epoch: [4] Test  [ 0/61]  eta: 0:01:49    time: 1.8008  data: 0.7429  max mem: 8262\n",
            "Epoch: [4] Test  [10/61]  eta: 0:00:53    time: 1.0442  data: 0.0677  max mem: 8262\n",
            "Epoch: [4] Test  [20/61]  eta: 0:00:44    time: 1.0608  data: 0.0002  max mem: 8262\n",
            "Epoch: [4] Test  [30/61]  eta: 0:00:33    time: 1.1136  data: 0.0003  max mem: 8262\n",
            "Epoch: [4] Test  [40/61]  eta: 0:00:22    time: 1.0020  data: 0.0003  max mem: 8262\n",
            "Epoch: [4] Test  [50/61]  eta: 0:00:11    time: 1.0357  data: 0.0003  max mem: 8262\n",
            "Epoch: [4] Test  [60/61]  eta: 0:00:01    time: 1.0562  data: 0.0003  max mem: 8262\n",
            "Epoch: [4] Test Total time: 0:01:04\n",
            "global correct: 95.5\n",
            "average row correct: ['97.0', '97.8', '92.7', '95.7', '88.4', '90.5', '98.3', '92.5', '97.0', '64.8', '94.5', '61.3', '93.7', '94.6', '96.0', '94.6', '78.9', '95.6', '78.2', '97.3', '89.0']\n",
            "IoU: ['94.8', '89.2', '42.6', '88.2', '66.8', '75.6', '96.6', '82.7', '93.0', '49.5', '91.6', '56.8', '89.0', '91.0', '91.2', '90.5', '68.8', '85.2', '63.5', '90.5', '78.4']\n",
            "mean IoU: 79.8\n",
            "Epoch: [5] Train  [  0/330]  eta: 1:00:44  lr: 0.000081  loss: 0.7667 (0.7667)  time: 11.0438  data: 0.5671  max mem: 8262\n",
            "Epoch: [5] Train  [ 10/330]  eta: 0:55:57  lr: 0.000081  loss: 0.3993 (0.5021)  time: 10.4928  data: 0.0518  max mem: 8262\n",
            "Epoch: [5] Train  [ 20/330]  eta: 0:54:10  lr: 0.000081  loss: 1.1855 (0.5506)  time: 10.4586  data: 0.0003  max mem: 8262\n",
            "Epoch: [5] Train  [ 30/330]  eta: 0:52:24  lr: 0.000080  loss: 0.3780 (0.4996)  time: 10.4735  data: 0.0003  max mem: 8262\n",
            "Epoch: [5] Train  [ 40/330]  eta: 0:50:35  lr: 0.000080  loss: 0.4266 (0.5006)  time: 10.4478  data: 0.0003  max mem: 8262\n",
            "Epoch: [5] Train  [ 50/330]  eta: 0:48:49  lr: 0.000080  loss: 0.8082 (0.4906)  time: 10.4378  data: 0.0003  max mem: 8262\n",
            "Epoch: [5] Train  [ 60/330]  eta: 0:47:05  lr: 0.000080  loss: 0.3030 (0.4826)  time: 10.4597  data: 0.0003  max mem: 8262\n",
            "Epoch: [5] Train  [ 70/330]  eta: 0:45:19  lr: 0.000080  loss: 0.6376 (0.5028)  time: 10.4520  data: 0.0002  max mem: 8262\n",
            "Epoch: [5] Train  [ 80/330]  eta: 0:43:37  lr: 0.000080  loss: 0.5553 (0.5306)  time: 10.4780  data: 0.0002  max mem: 8262\n",
            "Epoch: [5] Train  [ 90/330]  eta: 0:41:52  lr: 0.000079  loss: 0.4383 (0.5301)  time: 10.4965  data: 0.0002  max mem: 8262\n",
            "Epoch: [5] Train  [100/330]  eta: 0:40:08  lr: 0.000079  loss: 0.4805 (0.5331)  time: 10.4829  data: 0.0002  max mem: 8262\n",
            "Epoch: [5] Train  [110/330]  eta: 0:38:23  lr: 0.000079  loss: 0.6341 (0.5256)  time: 10.4825  data: 0.0002  max mem: 8262\n",
            "Epoch: [5] Train  [120/330]  eta: 0:36:39  lr: 0.000079  loss: 0.7644 (0.5373)  time: 10.4903  data: 0.0002  max mem: 8262\n",
            "Epoch: [5] Train  [130/330]  eta: 0:34:55  lr: 0.000079  loss: 0.8837 (0.5380)  time: 10.5014  data: 0.0002  max mem: 8262\n",
            "Epoch: [5] Train  [140/330]  eta: 0:33:10  lr: 0.000079  loss: 0.6226 (0.5357)  time: 10.4903  data: 0.0002  max mem: 8262\n",
            "Epoch: [5] Train  [150/330]  eta: 0:31:26  lr: 0.000079  loss: 0.1613 (0.5283)  time: 10.5073  data: 0.0002  max mem: 8262\n",
            "Epoch: [5] Train  [160/330]  eta: 0:29:41  lr: 0.000078  loss: 0.2735 (0.5252)  time: 10.5184  data: 0.0002  max mem: 8262\n",
            "Epoch: [5] Train  [170/330]  eta: 0:27:57  lr: 0.000078  loss: 0.1391 (0.5150)  time: 10.5241  data: 0.0002  max mem: 8262\n",
            "Epoch: [5] Train  [180/330]  eta: 0:26:12  lr: 0.000078  loss: 0.7432 (0.5095)  time: 10.5154  data: 0.0002  max mem: 8262\n",
            "Epoch: [5] Train  [190/330]  eta: 0:24:27  lr: 0.000078  loss: 0.2650 (0.5078)  time: 10.4771  data: 0.0002  max mem: 8262\n",
            "Epoch: [5] Train  [200/330]  eta: 0:22:42  lr: 0.000078  loss: 0.2575 (0.5043)  time: 10.4239  data: 0.0002  max mem: 8262\n",
            "Epoch: [5] Train  [210/330]  eta: 0:20:57  lr: 0.000078  loss: 0.3942 (0.4976)  time: 10.4167  data: 0.0002  max mem: 8262\n",
            "Epoch: [5] Train  [220/330]  eta: 0:19:12  lr: 0.000078  loss: 0.6563 (0.5006)  time: 10.4372  data: 0.0002  max mem: 8262\n",
            "Epoch: [5] Train  [230/330]  eta: 0:17:27  lr: 0.000077  loss: 0.5651 (0.4988)  time: 10.4693  data: 0.0003  max mem: 8262\n",
            "Epoch: [5] Train  [240/330]  eta: 0:15:42  lr: 0.000077  loss: 0.4270 (0.5043)  time: 10.4535  data: 0.0003  max mem: 8262\n",
            "Epoch: [5] Train  [250/330]  eta: 0:13:57  lr: 0.000077  loss: 0.4845 (0.4980)  time: 10.4479  data: 0.0002  max mem: 8262\n",
            "Epoch: [5] Train  [260/330]  eta: 0:12:13  lr: 0.000077  loss: 0.5833 (0.4933)  time: 10.5229  data: 0.0003  max mem: 8262\n",
            "Epoch: [5] Train  [270/330]  eta: 0:10:28  lr: 0.000077  loss: 0.2310 (0.4920)  time: 10.4800  data: 0.0003  max mem: 8262\n",
            "Epoch: [5] Train  [280/330]  eta: 0:08:43  lr: 0.000077  loss: 0.5314 (0.4901)  time: 10.4332  data: 0.0002  max mem: 8262\n",
            "Epoch: [5] Train  [290/330]  eta: 0:06:58  lr: 0.000077  loss: 0.1317 (0.4869)  time: 10.4478  data: 0.0002  max mem: 8262\n",
            "Epoch: [5] Train  [300/330]  eta: 0:05:14  lr: 0.000076  loss: 0.2117 (0.4866)  time: 10.4307  data: 0.0002  max mem: 8262\n",
            "Epoch: [5] Train  [310/330]  eta: 0:03:29  lr: 0.000076  loss: 0.1955 (0.4836)  time: 10.4512  data: 0.0002  max mem: 8262\n",
            "Epoch: [5] Train  [320/330]  eta: 0:01:44  lr: 0.000076  loss: 0.3711 (0.4815)  time: 10.4759  data: 0.0002  max mem: 8262\n",
            "Epoch: [5] Train Total time: 0:57:36\n",
            "Epoch: [5] Test  [ 0/61]  eta: 0:01:57    time: 1.9257  data: 0.8566  max mem: 8262\n",
            "Epoch: [5] Test  [10/61]  eta: 0:00:53    time: 1.0579  data: 0.0781  max mem: 8262\n",
            "Epoch: [5] Test  [20/61]  eta: 0:00:45    time: 1.0633  data: 0.0003  max mem: 8262\n",
            "Epoch: [5] Test  [30/61]  eta: 0:00:33    time: 1.1137  data: 0.0003  max mem: 8262\n",
            "Epoch: [5] Test  [40/61]  eta: 0:00:22    time: 1.0008  data: 0.0003  max mem: 8262\n",
            "Epoch: [5] Test  [50/61]  eta: 0:00:11    time: 1.0372  data: 0.0003  max mem: 8262\n",
            "Epoch: [5] Test  [60/61]  eta: 0:00:01    time: 1.0564  data: 0.0002  max mem: 8262\n",
            "Epoch: [5] Test Total time: 0:01:04\n",
            "global correct: 95.5\n",
            "average row correct: ['96.8', '97.7', '91.2', '95.3', '87.4', '94.6', '98.4', '92.5', '97.4', '59.5', '96.3', '61.7', '96.0', '96.1', '96.1', '95.1', '86.8', '95.2', '82.5', '97.1', '88.4']\n",
            "IoU: ['94.8', '91.8', '42.5', '89.3', '66.3', '74.3', '96.6', '83.8', '93.5', '46.2', '93.6', '57.1', '89.6', '91.9', '91.1', '90.8', '66.1', '89.9', '63.3', '89.9', '80.3']\n",
            "mean IoU: 80.1\n",
            "Epoch: [6] Train  [  0/330]  eta: 1:00:07  lr: 0.000076  loss: 0.7297 (0.7297)  time: 10.9324  data: 0.5206  max mem: 8262\n",
            "Epoch: [6] Train  [ 10/330]  eta: 0:56:05  lr: 0.000076  loss: 0.7736 (0.4327)  time: 10.5172  data: 0.0475  max mem: 8262\n",
            "Epoch: [6] Train  [ 20/330]  eta: 0:54:16  lr: 0.000076  loss: 0.7397 (0.4741)  time: 10.4822  data: 0.0002  max mem: 8262\n",
            "Epoch: [6] Train  [ 30/330]  eta: 0:52:24  lr: 0.000076  loss: 0.2656 (0.4514)  time: 10.4634  data: 0.0002  max mem: 8262\n",
            "Epoch: [6] Train  [ 40/330]  eta: 0:50:36  lr: 0.000075  loss: 0.1292 (0.4462)  time: 10.4349  data: 0.0004  max mem: 8262\n",
            "Epoch: [6] Train  [ 50/330]  eta: 0:48:48  lr: 0.000075  loss: 0.4511 (0.4990)  time: 10.4225  data: 0.0006  max mem: 8262\n",
            "Epoch: [6] Train  [ 60/330]  eta: 0:47:02  lr: 0.000075  loss: 0.4072 (0.5276)  time: 10.4178  data: 0.0005  max mem: 8262\n",
            "Epoch: [6] Train  [ 70/330]  eta: 0:45:17  lr: 0.000075  loss: 0.2772 (0.5102)  time: 10.4342  data: 0.0004  max mem: 8262\n",
            "Epoch: [6] Train  [ 80/330]  eta: 0:43:32  lr: 0.000075  loss: 1.1237 (0.5081)  time: 10.4396  data: 0.0004  max mem: 8262\n",
            "Epoch: [6] Train  [ 90/330]  eta: 0:41:49  lr: 0.000075  loss: 0.2262 (0.5031)  time: 10.4705  data: 0.0003  max mem: 8262\n",
            "Epoch: [6] Train  [100/330]  eta: 0:40:04  lr: 0.000074  loss: 0.1430 (0.4929)  time: 10.4644  data: 0.0003  max mem: 8262\n",
            "Epoch: [6] Train  [110/330]  eta: 0:38:19  lr: 0.000074  loss: 1.1928 (0.5052)  time: 10.4226  data: 0.0002  max mem: 8262\n",
            "Epoch: [6] Train  [120/330]  eta: 0:36:34  lr: 0.000074  loss: 0.2453 (0.5013)  time: 10.4261  data: 0.0003  max mem: 8262\n",
            "Epoch: [6] Train  [130/330]  eta: 0:34:50  lr: 0.000074  loss: 0.2074 (0.5115)  time: 10.4677  data: 0.0003  max mem: 8262\n",
            "Epoch: [6] Train  [140/330]  eta: 0:33:06  lr: 0.000074  loss: 0.6147 (0.5069)  time: 10.4949  data: 0.0003  max mem: 8262\n",
            "Epoch: [6] Train  [150/330]  eta: 0:31:21  lr: 0.000074  loss: 0.1707 (0.5072)  time: 10.4704  data: 0.0002  max mem: 8262\n",
            "Epoch: [6] Train  [160/330]  eta: 0:29:37  lr: 0.000074  loss: 0.2789 (0.5116)  time: 10.4537  data: 0.0002  max mem: 8262\n",
            "Epoch: [6] Train  [170/330]  eta: 0:27:52  lr: 0.000073  loss: 0.3982 (0.5101)  time: 10.4215  data: 0.0002  max mem: 8262\n",
            "Epoch: [6] Train  [180/330]  eta: 0:26:07  lr: 0.000073  loss: 0.1777 (0.5043)  time: 10.4359  data: 0.0002  max mem: 8262\n",
            "Epoch: [6] Train  [190/330]  eta: 0:24:23  lr: 0.000073  loss: 0.3168 (0.4988)  time: 10.4668  data: 0.0003  max mem: 8262\n",
            "Epoch: [6] Train  [200/330]  eta: 0:22:39  lr: 0.000073  loss: 0.3167 (0.4935)  time: 10.4781  data: 0.0003  max mem: 8262\n",
            "Epoch: [6] Train  [210/330]  eta: 0:20:54  lr: 0.000073  loss: 0.5659 (0.4924)  time: 10.4839  data: 0.0003  max mem: 8262\n",
            "Epoch: [6] Train  [220/330]  eta: 0:19:10  lr: 0.000073  loss: 0.1633 (0.5017)  time: 10.4756  data: 0.0003  max mem: 8262\n",
            "Epoch: [6] Train  [230/330]  eta: 0:17:25  lr: 0.000073  loss: 0.7821 (0.4962)  time: 10.4587  data: 0.0003  max mem: 8262\n",
            "Epoch: [6] Train  [240/330]  eta: 0:15:40  lr: 0.000072  loss: 0.4274 (0.4959)  time: 10.4128  data: 0.0003  max mem: 8262\n",
            "Epoch: [6] Train  [250/330]  eta: 0:13:56  lr: 0.000072  loss: 0.9569 (0.4951)  time: 10.4006  data: 0.0002  max mem: 8262\n",
            "Epoch: [6] Train  [260/330]  eta: 0:12:11  lr: 0.000072  loss: 0.4054 (0.4924)  time: 10.4241  data: 0.0002  max mem: 8262\n",
            "Epoch: [6] Train  [270/330]  eta: 0:10:27  lr: 0.000072  loss: 0.3741 (0.4900)  time: 10.4729  data: 0.0003  max mem: 8262\n",
            "Epoch: [6] Train  [280/330]  eta: 0:08:42  lr: 0.000072  loss: 0.2698 (0.4860)  time: 10.4937  data: 0.0003  max mem: 8262\n",
            "Epoch: [6] Train  [290/330]  eta: 0:06:58  lr: 0.000072  loss: 0.6593 (0.4822)  time: 10.4971  data: 0.0003  max mem: 8262\n",
            "Epoch: [6] Train  [300/330]  eta: 0:05:13  lr: 0.000071  loss: 0.3896 (0.4841)  time: 10.4885  data: 0.0003  max mem: 8262\n",
            "Epoch: [6] Train  [310/330]  eta: 0:03:29  lr: 0.000071  loss: 0.3275 (0.4813)  time: 10.4705  data: 0.0003  max mem: 8262\n",
            "Epoch: [6] Train  [320/330]  eta: 0:01:44  lr: 0.000071  loss: 0.3586 (0.4819)  time: 10.4998  data: 0.0002  max mem: 8262\n",
            "Epoch: [6] Train Total time: 0:57:31\n",
            "Epoch: [6] Test  [ 0/61]  eta: 0:01:45    time: 1.7247  data: 0.6462  max mem: 8262\n",
            "Epoch: [6] Test  [10/61]  eta: 0:00:52    time: 1.0355  data: 0.0590  max mem: 8262\n",
            "Epoch: [6] Test  [20/61]  eta: 0:00:44    time: 1.0602  data: 0.0002  max mem: 8262\n",
            "Epoch: [6] Test  [30/61]  eta: 0:00:33    time: 1.1136  data: 0.0003  max mem: 8262\n",
            "Epoch: [6] Test  [40/61]  eta: 0:00:22    time: 1.0013  data: 0.0003  max mem: 8262\n",
            "Epoch: [6] Test  [50/61]  eta: 0:00:11    time: 1.0348  data: 0.0003  max mem: 8262\n",
            "Epoch: [6] Test  [60/61]  eta: 0:00:01    time: 1.0550  data: 0.0003  max mem: 8262\n",
            "Epoch: [6] Test Total time: 0:01:04\n",
            "global correct: 95.6\n",
            "average row correct: ['97.3', '98.2', '88.4', '95.0', '87.3', '89.8', '98.3', '90.3', '94.6', '63.5', '96.1', '60.5', '93.4', '97.1', '96.3', '95.3', '81.5', '94.3', '77.2', '96.3', '88.1']\n",
            "IoU: ['94.8', '89.7', '42.4', '89.3', '65.8', '77.0', '96.8', '82.5', '92.5', '48.4', '94.2', '56.1', '89.7', '89.8', '91.0', '90.9', '68.7', '89.8', '63.2', '90.6', '79.1']\n",
            "mean IoU: 80.1\n",
            "Epoch: [7] Train  [  0/330]  eta: 0:56:25  lr: 0.000071  loss: 0.5861 (0.5861)  time: 10.2578  data: 0.5441  max mem: 8262\n",
            "Epoch: [7] Train  [ 10/330]  eta: 0:55:33  lr: 0.000071  loss: 0.5298 (0.4389)  time: 10.4165  data: 0.0497  max mem: 8262\n",
            "Epoch: [7] Train  [ 20/330]  eta: 0:53:57  lr: 0.000071  loss: 0.6536 (0.4324)  time: 10.4520  data: 0.0002  max mem: 8262\n",
            "Epoch: [7] Train  [ 30/330]  eta: 0:52:15  lr: 0.000071  loss: 1.1523 (0.4683)  time: 10.4692  data: 0.0002  max mem: 8262\n",
            "Epoch: [7] Train  [ 40/330]  eta: 0:50:34  lr: 0.000070  loss: 0.3621 (0.4650)  time: 10.4883  data: 0.0002  max mem: 8262\n",
            "Epoch: [7] Train  [ 50/330]  eta: 0:48:51  lr: 0.000070  loss: 0.4479 (0.4656)  time: 10.5005  data: 0.0003  max mem: 8262\n",
            "Epoch: [7] Train  [ 60/330]  eta: 0:47:03  lr: 0.000070  loss: 0.8593 (0.4551)  time: 10.4412  data: 0.0003  max mem: 8262\n",
            "Epoch: [7] Train  [ 70/330]  eta: 0:45:17  lr: 0.000070  loss: 0.2523 (0.4414)  time: 10.4072  data: 0.0002  max mem: 8262\n",
            "Epoch: [7] Train  [ 80/330]  eta: 0:43:34  lr: 0.000070  loss: 0.2764 (0.4431)  time: 10.4571  data: 0.0002  max mem: 8262\n",
            "Epoch: [7] Train  [ 90/330]  eta: 0:41:51  lr: 0.000070  loss: 0.2503 (0.4330)  time: 10.4993  data: 0.0003  max mem: 8262\n",
            "Epoch: [7] Train  [100/330]  eta: 0:40:04  lr: 0.000070  loss: 0.2233 (0.4441)  time: 10.4436  data: 0.0003  max mem: 8262\n",
            "Epoch: [7] Train  [110/330]  eta: 0:38:20  lr: 0.000069  loss: 0.3640 (0.4517)  time: 10.4297  data: 0.0003  max mem: 8262\n",
            "Epoch: [7] Train  [120/330]  eta: 0:36:35  lr: 0.000069  loss: 0.2693 (0.4534)  time: 10.4689  data: 0.0002  max mem: 8262\n",
            "Epoch: [7] Train  [130/330]  eta: 0:34:50  lr: 0.000069  loss: 0.6338 (0.4701)  time: 10.4408  data: 0.0002  max mem: 8262\n",
            "Epoch: [7] Train  [140/330]  eta: 0:33:05  lr: 0.000069  loss: 0.8194 (0.4682)  time: 10.4157  data: 0.0003  max mem: 8262\n",
            "Epoch: [7] Train  [150/330]  eta: 0:31:21  lr: 0.000069  loss: 0.3853 (0.4714)  time: 10.4586  data: 0.0003  max mem: 8262\n",
            "Epoch: [7] Train  [160/330]  eta: 0:29:37  lr: 0.000069  loss: 0.3128 (0.4653)  time: 10.4772  data: 0.0003  max mem: 8262\n",
            "Epoch: [7] Train  [170/330]  eta: 0:27:52  lr: 0.000069  loss: 0.1911 (0.4580)  time: 10.4638  data: 0.0003  max mem: 8262\n",
            "Epoch: [7] Train  [180/330]  eta: 0:26:08  lr: 0.000068  loss: 1.8648 (0.4651)  time: 10.4725  data: 0.0003  max mem: 8262\n",
            "Epoch: [7] Train  [190/330]  eta: 0:24:24  lr: 0.000068  loss: 0.6641 (0.4684)  time: 10.4957  data: 0.0002  max mem: 8262\n",
            "Epoch: [7] Train  [200/330]  eta: 0:22:39  lr: 0.000068  loss: 0.4557 (0.4688)  time: 10.5016  data: 0.0003  max mem: 8262\n",
            "Epoch: [7] Train  [210/330]  eta: 0:20:55  lr: 0.000068  loss: 0.4624 (0.4688)  time: 10.5017  data: 0.0003  max mem: 8262\n",
            "Epoch: [7] Train  [220/330]  eta: 0:19:11  lr: 0.000068  loss: 0.1934 (0.4649)  time: 10.4989  data: 0.0002  max mem: 8262\n",
            "Epoch: [7] Train  [230/330]  eta: 0:17:26  lr: 0.000068  loss: 0.4966 (0.4675)  time: 10.4687  data: 0.0003  max mem: 8262\n",
            "Epoch: [7] Train  [240/330]  eta: 0:15:41  lr: 0.000067  loss: 1.7110 (0.4699)  time: 10.4445  data: 0.0003  max mem: 8262\n",
            "Epoch: [7] Train  [250/330]  eta: 0:13:56  lr: 0.000067  loss: 0.5839 (0.4676)  time: 10.4123  data: 0.0002  max mem: 8262\n",
            "Epoch: [7] Train  [260/330]  eta: 0:12:12  lr: 0.000067  loss: 0.5830 (0.4758)  time: 10.4452  data: 0.0003  max mem: 8262\n",
            "Epoch: [7] Train  [270/330]  eta: 0:10:27  lr: 0.000067  loss: 0.5657 (0.4856)  time: 10.5016  data: 0.0003  max mem: 8262\n",
            "Epoch: [7] Train  [280/330]  eta: 0:08:43  lr: 0.000067  loss: 0.5229 (0.4818)  time: 10.5476  data: 0.0002  max mem: 8262\n",
            "Epoch: [7] Train  [290/330]  eta: 0:06:58  lr: 0.000067  loss: 0.8184 (0.4830)  time: 10.5066  data: 0.0003  max mem: 8262\n",
            "Epoch: [7] Train  [300/330]  eta: 0:05:13  lr: 0.000067  loss: 0.2975 (0.4817)  time: 10.4001  data: 0.0003  max mem: 8262\n",
            "Epoch: [7] Train  [310/330]  eta: 0:03:29  lr: 0.000066  loss: 0.3012 (0.4886)  time: 10.4394  data: 0.0003  max mem: 8262\n",
            "Epoch: [7] Train  [320/330]  eta: 0:01:44  lr: 0.000066  loss: 0.7873 (0.4876)  time: 10.4537  data: 0.0003  max mem: 8262\n",
            "Epoch: [7] Train Total time: 0:57:32\n",
            "Epoch: [7] Test  [ 0/61]  eta: 0:01:53    time: 1.8555  data: 0.7905  max mem: 8262\n",
            "Epoch: [7] Test  [10/61]  eta: 0:00:53    time: 1.0520  data: 0.0721  max mem: 8262\n",
            "Epoch: [7] Test  [20/61]  eta: 0:00:45    time: 1.0642  data: 0.0003  max mem: 8262\n",
            "Epoch: [7] Test  [30/61]  eta: 0:00:33    time: 1.1163  data: 0.0003  max mem: 8262\n",
            "Epoch: [7] Test  [40/61]  eta: 0:00:22    time: 1.0022  data: 0.0003  max mem: 8262\n",
            "Epoch: [7] Test  [50/61]  eta: 0:00:11    time: 1.0359  data: 0.0003  max mem: 8262\n",
            "Epoch: [7] Test  [60/61]  eta: 0:00:01    time: 1.0559  data: 0.0002  max mem: 8262\n",
            "Epoch: [7] Test Total time: 0:01:04\n",
            "global correct: 95.4\n",
            "average row correct: ['96.5', '98.6', '92.9', '97.1', '89.4', '94.9', '98.5', '94.2', '98.4', '67.1', '94.7', '61.9', '95.1', '96.6', '97.1', '94.2', '87.3', '95.8', '80.4', '97.7', '89.4']\n",
            "IoU: ['94.6', '88.9', '41.9', '87.5', '62.0', '75.4', '96.6', '83.3', '91.5', '48.7', '92.5', '57.3', '88.4', '92.1', '91.1', '90.7', '65.1', '86.7', '64.8', '91.5', '78.9']\n",
            "mean IoU: 79.5\n",
            "Epoch: [8] Train  [  0/330]  eta: 1:00:58  lr: 0.000066  loss: 0.5251 (0.5251)  time: 11.0849  data: 0.6503  max mem: 8262\n",
            "Epoch: [8] Train  [ 10/330]  eta: 0:56:40  lr: 0.000066  loss: 0.2948 (0.5367)  time: 10.6278  data: 0.0593  max mem: 8262\n",
            "Epoch: [8] Train  [ 20/330]  eta: 0:54:29  lr: 0.000066  loss: 0.6677 (0.5314)  time: 10.5194  data: 0.0002  max mem: 8262\n",
            "Epoch: [8] Train  [ 30/330]  eta: 0:52:39  lr: 0.000066  loss: 0.4678 (0.5083)  time: 10.4808  data: 0.0002  max mem: 8262\n",
            "Epoch: [8] Train  [ 40/330]  eta: 0:50:46  lr: 0.000066  loss: 0.3219 (0.4998)  time: 10.4599  data: 0.0002  max mem: 8262\n",
            "Epoch: [8] Train  [ 50/330]  eta: 0:48:58  lr: 0.000065  loss: 0.2114 (0.4856)  time: 10.4340  data: 0.0002  max mem: 8262\n",
            "Epoch: [8] Train  [ 60/330]  eta: 0:47:11  lr: 0.000065  loss: 0.6754 (0.4878)  time: 10.4528  data: 0.0002  max mem: 8262\n",
            "Epoch: [8] Train  [ 70/330]  eta: 0:45:24  lr: 0.000065  loss: 0.2749 (0.4969)  time: 10.4385  data: 0.0002  max mem: 8262\n",
            "Epoch: [8] Train  [ 80/330]  eta: 0:43:38  lr: 0.000065  loss: 0.4743 (0.5011)  time: 10.4388  data: 0.0002  max mem: 8262\n",
            "Epoch: [8] Train  [ 90/330]  eta: 0:41:53  lr: 0.000065  loss: 0.3705 (0.4991)  time: 10.4443  data: 0.0002  max mem: 8262\n",
            "Epoch: [8] Train  [100/330]  eta: 0:40:07  lr: 0.000065  loss: 0.5060 (0.4881)  time: 10.4324  data: 0.0003  max mem: 8262\n",
            "Epoch: [8] Train  [110/330]  eta: 0:38:23  lr: 0.000064  loss: 0.3500 (0.4764)  time: 10.4576  data: 0.0004  max mem: 8262\n",
            "Epoch: [8] Train  [120/330]  eta: 0:36:37  lr: 0.000064  loss: 0.6105 (0.4691)  time: 10.4396  data: 0.0004  max mem: 8262\n",
            "Epoch: [8] Train  [130/330]  eta: 0:34:52  lr: 0.000064  loss: 0.3667 (0.4690)  time: 10.4337  data: 0.0003  max mem: 8262\n",
            "Epoch: [8] Train  [140/330]  eta: 0:33:08  lr: 0.000064  loss: 0.3337 (0.4638)  time: 10.4853  data: 0.0003  max mem: 8262\n",
            "Epoch: [8] Train  [150/330]  eta: 0:31:24  lr: 0.000064  loss: 0.3111 (0.4687)  time: 10.5066  data: 0.0002  max mem: 8262\n",
            "Epoch: [8] Train  [160/330]  eta: 0:29:39  lr: 0.000064  loss: 0.3230 (0.4684)  time: 10.4942  data: 0.0002  max mem: 8262\n",
            "Epoch: [8] Train  [170/330]  eta: 0:27:54  lr: 0.000064  loss: 0.2382 (0.4682)  time: 10.4275  data: 0.0002  max mem: 8262\n",
            "Epoch: [8] Train  [180/330]  eta: 0:26:09  lr: 0.000063  loss: 1.4184 (0.4711)  time: 10.4181  data: 0.0002  max mem: 8262\n",
            "Epoch: [8] Train  [190/330]  eta: 0:24:24  lr: 0.000063  loss: 0.3448 (0.4702)  time: 10.4603  data: 0.0002  max mem: 8262\n",
            "Epoch: [8] Train  [200/330]  eta: 0:22:40  lr: 0.000063  loss: 0.1374 (0.4746)  time: 10.4690  data: 0.0002  max mem: 8262\n",
            "Epoch: [8] Train  [210/330]  eta: 0:20:55  lr: 0.000063  loss: 0.7403 (0.4741)  time: 10.4620  data: 0.0003  max mem: 8262\n",
            "Epoch: [8] Train  [220/330]  eta: 0:19:10  lr: 0.000063  loss: 0.3045 (0.4748)  time: 10.4437  data: 0.0003  max mem: 8262\n",
            "Epoch: [8] Train  [230/330]  eta: 0:17:26  lr: 0.000063  loss: 0.8383 (0.4750)  time: 10.4405  data: 0.0003  max mem: 8262\n",
            "Epoch: [8] Train  [240/330]  eta: 0:15:41  lr: 0.000062  loss: 0.2312 (0.4757)  time: 10.4701  data: 0.0002  max mem: 8262\n",
            "Epoch: [8] Train  [250/330]  eta: 0:13:57  lr: 0.000062  loss: 0.4390 (0.4712)  time: 10.4997  data: 0.0002  max mem: 8262\n",
            "Epoch: [8] Train  [260/330]  eta: 0:12:12  lr: 0.000062  loss: 0.1297 (0.4728)  time: 10.4861  data: 0.0002  max mem: 8262\n",
            "Epoch: [8] Train  [270/330]  eta: 0:10:27  lr: 0.000062  loss: 0.1114 (0.4717)  time: 10.4514  data: 0.0002  max mem: 8262\n",
            "Epoch: [8] Train  [280/330]  eta: 0:08:43  lr: 0.000062  loss: 0.3760 (0.4744)  time: 10.4352  data: 0.0002  max mem: 8262\n",
            "Epoch: [8] Train  [290/330]  eta: 0:06:58  lr: 0.000062  loss: 0.4080 (0.4742)  time: 10.4512  data: 0.0002  max mem: 8262\n",
            "Epoch: [8] Train  [300/330]  eta: 0:05:13  lr: 0.000062  loss: 0.1464 (0.4719)  time: 10.4579  data: 0.0002  max mem: 8262\n",
            "Epoch: [8] Train  [310/330]  eta: 0:03:29  lr: 0.000061  loss: 0.7518 (0.4708)  time: 10.4784  data: 0.0003  max mem: 8262\n",
            "Epoch: [8] Train  [320/330]  eta: 0:01:44  lr: 0.000061  loss: 0.1582 (0.4680)  time: 10.4644  data: 0.0002  max mem: 8262\n",
            "Epoch: [8] Train Total time: 0:57:33\n",
            "Epoch: [8] Test  [ 0/61]  eta: 0:01:47    time: 1.7629  data: 0.6712  max mem: 8262\n",
            "Epoch: [8] Test  [10/61]  eta: 0:00:53    time: 1.0407  data: 0.0615  max mem: 8262\n",
            "Epoch: [8] Test  [20/61]  eta: 0:00:44    time: 1.0637  data: 0.0004  max mem: 8262\n",
            "Epoch: [8] Test  [30/61]  eta: 0:00:33    time: 1.1177  data: 0.0003  max mem: 8262\n",
            "Epoch: [8] Test  [40/61]  eta: 0:00:22    time: 1.0046  data: 0.0003  max mem: 8262\n",
            "Epoch: [8] Test  [50/61]  eta: 0:00:11    time: 1.0371  data: 0.0003  max mem: 8262\n",
            "Epoch: [8] Test  [60/61]  eta: 0:00:01    time: 1.0569  data: 0.0002  max mem: 8262\n",
            "Epoch: [8] Test Total time: 0:01:04\n",
            "global correct: 95.4\n",
            "average row correct: ['96.6', '98.6', '92.9', '97.2', '90.5', '94.9', '98.8', '92.8', '97.3', '65.1', '96.2', '61.6', '96.5', '94.1', '96.2', '94.0', '83.2', '96.1', '79.4', '97.8', '89.7']\n",
            "IoU: ['94.6', '89.5', '42.1', '87.4', '61.5', '75.4', '96.7', '83.9', '93.7', '47.8', '91.5', '57.0', '90.0', '90.3', '91.2', '90.7', '66.7', '87.2', '63.4', '91.8', '75.4']\n",
            "mean IoU: 79.4\n",
            "Epoch: [9] Train  [  0/330]  eta: 0:58:41  lr: 0.000061  loss: 0.1540 (0.1540)  time: 10.6700  data: 0.5993  max mem: 8262\n",
            "Epoch: [9] Train  [ 10/330]  eta: 0:55:55  lr: 0.000061  loss: 0.6181 (0.4482)  time: 10.4846  data: 0.0547  max mem: 8262\n",
            "Epoch: [9] Train  [ 20/330]  eta: 0:54:07  lr: 0.000061  loss: 0.5482 (0.5911)  time: 10.4653  data: 0.0002  max mem: 8262\n",
            "Epoch: [9] Train  [ 30/330]  eta: 0:52:29  lr: 0.000061  loss: 0.2830 (0.5309)  time: 10.5060  data: 0.0002  max mem: 8262\n",
            "Epoch: [9] Train  [ 40/330]  eta: 0:50:42  lr: 0.000061  loss: 0.5995 (0.5265)  time: 10.5090  data: 0.0002  max mem: 8262\n",
            "Epoch: [9] Train  [ 50/330]  eta: 0:48:55  lr: 0.000060  loss: 0.2978 (0.5454)  time: 10.4589  data: 0.0003  max mem: 8262\n",
            "Epoch: [9] Train  [ 60/330]  eta: 0:47:08  lr: 0.000060  loss: 1.6827 (0.5484)  time: 10.4417  data: 0.0002  max mem: 8262\n",
            "Epoch: [9] Train  [ 70/330]  eta: 0:45:24  lr: 0.000060  loss: 0.7087 (0.5375)  time: 10.4668  data: 0.0003  max mem: 8262\n",
            "Epoch: [9] Train  [ 80/330]  eta: 0:43:39  lr: 0.000060  loss: 0.3866 (0.5474)  time: 10.4839  data: 0.0003  max mem: 8262\n",
            "Epoch: [9] Train  [ 90/330]  eta: 0:41:56  lr: 0.000060  loss: 0.3612 (0.5352)  time: 10.5017  data: 0.0002  max mem: 8262\n",
            "Epoch: [9] Train  [100/330]  eta: 0:40:12  lr: 0.000060  loss: 0.1922 (0.5262)  time: 10.5335  data: 0.0002  max mem: 8262\n",
            "Epoch: [9] Train  [110/330]  eta: 0:38:26  lr: 0.000059  loss: 0.2204 (0.5232)  time: 10.4974  data: 0.0003  max mem: 8262\n",
            "Epoch: [9] Train  [120/330]  eta: 0:36:41  lr: 0.000059  loss: 0.4033 (0.5110)  time: 10.4465  data: 0.0003  max mem: 8262\n",
            "Epoch: [9] Train  [130/330]  eta: 0:34:55  lr: 0.000059  loss: 0.2785 (0.4996)  time: 10.4317  data: 0.0002  max mem: 8262\n",
            "Epoch: [9] Train  [140/330]  eta: 0:33:10  lr: 0.000059  loss: 1.1556 (0.5080)  time: 10.4445  data: 0.0002  max mem: 8262\n",
            "Epoch: [9] Train  [150/330]  eta: 0:31:25  lr: 0.000059  loss: 0.5670 (0.5045)  time: 10.4621  data: 0.0002  max mem: 8262\n",
            "Epoch: [9] Train  [160/330]  eta: 0:29:41  lr: 0.000059  loss: 0.8150 (0.5050)  time: 10.4848  data: 0.0002  max mem: 8262\n",
            "Epoch: [9] Train  [170/330]  eta: 0:27:55  lr: 0.000059  loss: 0.4071 (0.5043)  time: 10.4461  data: 0.0002  max mem: 8262\n",
            "Epoch: [9] Train  [180/330]  eta: 0:26:10  lr: 0.000058  loss: 0.3458 (0.4997)  time: 10.4231  data: 0.0003  max mem: 8262\n",
            "Epoch: [9] Train  [190/330]  eta: 0:24:26  lr: 0.000058  loss: 0.5700 (0.4947)  time: 10.4717  data: 0.0002  max mem: 8262\n",
            "Epoch: [9] Train  [200/330]  eta: 0:22:41  lr: 0.000058  loss: 0.4876 (0.5033)  time: 10.4972  data: 0.0002  max mem: 8262\n",
            "Epoch: [9] Train  [210/330]  eta: 0:20:56  lr: 0.000058  loss: 0.6615 (0.5055)  time: 10.4892  data: 0.0002  max mem: 8262\n",
            "Epoch: [9] Train  [220/330]  eta: 0:19:12  lr: 0.000058  loss: 0.1666 (0.5048)  time: 10.4617  data: 0.0002  max mem: 8262\n",
            "Epoch: [9] Train  [230/330]  eta: 0:17:27  lr: 0.000058  loss: 0.5537 (0.5063)  time: 10.4598  data: 0.0002  max mem: 8262\n",
            "Epoch: [9] Train  [240/330]  eta: 0:15:42  lr: 0.000057  loss: 0.2274 (0.5073)  time: 10.4912  data: 0.0002  max mem: 8262\n",
            "Epoch: [9] Train  [250/330]  eta: 0:13:57  lr: 0.000057  loss: 0.4658 (0.5086)  time: 10.4815  data: 0.0002  max mem: 8262\n",
            "Epoch: [9] Train  [260/330]  eta: 0:12:13  lr: 0.000057  loss: 0.3546 (0.5059)  time: 10.5110  data: 0.0002  max mem: 8262\n",
            "Epoch: [9] Train  [270/330]  eta: 0:10:28  lr: 0.000057  loss: 0.5924 (0.5089)  time: 10.5021  data: 0.0002  max mem: 8262\n",
            "Epoch: [9] Train  [280/330]  eta: 0:08:43  lr: 0.000057  loss: 0.1007 (0.5118)  time: 10.4260  data: 0.0002  max mem: 8262\n",
            "Epoch: [9] Train  [290/330]  eta: 0:06:59  lr: 0.000057  loss: 0.5131 (0.5046)  time: 10.4869  data: 0.0002  max mem: 8262\n",
            "Epoch: [9] Train  [300/330]  eta: 0:05:14  lr: 0.000057  loss: 1.7341 (0.5075)  time: 10.5315  data: 0.0002  max mem: 8262\n",
            "Epoch: [9] Train  [310/330]  eta: 0:03:29  lr: 0.000056  loss: 0.4509 (0.5090)  time: 10.5035  data: 0.0002  max mem: 8262\n",
            "Epoch: [9] Train  [320/330]  eta: 0:01:44  lr: 0.000056  loss: 0.1322 (0.5076)  time: 10.4976  data: 0.0002  max mem: 8262\n",
            "Epoch: [9] Train Total time: 0:57:39\n",
            "Epoch: [9] Test  [ 0/61]  eta: 0:01:42    time: 1.6835  data: 0.6185  max mem: 8262\n",
            "Epoch: [9] Test  [10/61]  eta: 0:00:52    time: 1.0364  data: 0.0566  max mem: 8262\n",
            "Epoch: [9] Test  [20/61]  eta: 0:00:44    time: 1.0635  data: 0.0003  max mem: 8262\n",
            "Epoch: [9] Test  [30/61]  eta: 0:00:33    time: 1.1164  data: 0.0003  max mem: 8262\n",
            "Epoch: [9] Test  [40/61]  eta: 0:00:22    time: 1.0044  data: 0.0003  max mem: 8262\n",
            "Epoch: [9] Test  [50/61]  eta: 0:00:11    time: 1.0367  data: 0.0003  max mem: 8262\n",
            "Epoch: [9] Test  [60/61]  eta: 0:00:01    time: 1.0552  data: 0.0002  max mem: 8262\n",
            "Epoch: [9] Test Total time: 0:01:04\n",
            "global correct: 95.5\n",
            "average row correct: ['96.9', '98.3', '92.5', '94.6', '87.9', '94.2', '98.4', '91.8', '97.8', '65.0', '95.2', '61.9', '94.6', '93.4', '96.0', '94.8', '85.9', '95.8', '79.0', '97.0', '88.2']\n",
            "IoU: ['94.8', '90.8', '42.3', '89.8', '65.8', '75.3', '96.8', '84.7', '92.4', '47.0', '90.8', '57.3', '88.6', '90.0', '91.6', '90.9', '67.9', '87.0', '63.4', '91.3', '78.5']\n",
            "mean IoU: 79.8\n",
            "Epoch: [10] Train  [  0/330]  eta: 0:59:45  lr: 0.000056  loss: 0.1560 (0.1560)  time: 10.8651  data: 0.6053  max mem: 8262\n",
            "Epoch: [10] Train  [ 10/330]  eta: 0:56:04  lr: 0.000056  loss: 0.4657 (0.2994)  time: 10.5153  data: 0.0553  max mem: 8262\n",
            "Epoch: [10] Train  [ 20/330]  eta: 0:54:22  lr: 0.000056  loss: 0.1721 (0.3082)  time: 10.5080  data: 0.0002  max mem: 8262\n",
            "Epoch: [10] Train  [ 30/330]  eta: 0:52:45  lr: 0.000056  loss: 0.3932 (0.4221)  time: 10.5740  data: 0.0002  max mem: 8262\n",
            "Epoch: [10] Train  [ 40/330]  eta: 0:50:52  lr: 0.000055  loss: 0.3324 (0.4360)  time: 10.5250  data: 0.0002  max mem: 8262\n",
            "Epoch: [10] Train  [ 50/330]  eta: 0:49:08  lr: 0.000055  loss: 0.2442 (0.4336)  time: 10.4941  data: 0.0002  max mem: 8262\n",
            "Epoch: [10] Train  [ 60/330]  eta: 0:47:20  lr: 0.000055  loss: 0.4296 (0.4321)  time: 10.5116  data: 0.0003  max mem: 8262\n",
            "Epoch: [10] Train  [ 70/330]  eta: 0:45:36  lr: 0.000055  loss: 0.4683 (0.4366)  time: 10.5140  data: 0.0002  max mem: 8262\n",
            "Epoch: [10] Train  [ 80/330]  eta: 0:43:47  lr: 0.000055  loss: 0.5875 (0.4331)  time: 10.4855  data: 0.0002  max mem: 8262\n",
            "Epoch: [10] Train  [ 90/330]  eta: 0:42:03  lr: 0.000055  loss: 0.5251 (0.4402)  time: 10.4732  data: 0.0002  max mem: 8262\n",
            "Epoch: [10] Train  [100/330]  eta: 0:40:17  lr: 0.000055  loss: 0.2696 (0.4495)  time: 10.5097  data: 0.0002  max mem: 8262\n",
            "Epoch: [10] Train  [110/330]  eta: 0:38:34  lr: 0.000054  loss: 0.3051 (0.4469)  time: 10.5408  data: 0.0002  max mem: 8262\n",
            "Epoch: [10] Train  [120/330]  eta: 0:36:50  lr: 0.000054  loss: 0.9379 (0.4546)  time: 10.5912  data: 0.0002  max mem: 8262\n",
            "Epoch: [10] Train  [130/330]  eta: 0:35:05  lr: 0.000054  loss: 0.2805 (0.4512)  time: 10.5611  data: 0.0002  max mem: 8262\n",
            "Epoch: [10] Train  [140/330]  eta: 0:33:19  lr: 0.000054  loss: 0.8259 (0.4649)  time: 10.5341  data: 0.0002  max mem: 8262\n",
            "Epoch: [10] Train  [150/330]  eta: 0:31:35  lr: 0.000054  loss: 0.8501 (0.4692)  time: 10.5466  data: 0.0002  max mem: 8262\n",
            "Epoch: [10] Train  [160/330]  eta: 0:29:49  lr: 0.000054  loss: 0.7100 (0.4697)  time: 10.5376  data: 0.0002  max mem: 8262\n",
            "Epoch: [10] Train  [170/330]  eta: 0:28:04  lr: 0.000053  loss: 0.2268 (0.4657)  time: 10.5358  data: 0.0002  max mem: 8262\n",
            "Epoch: [10] Train  [180/330]  eta: 0:26:19  lr: 0.000053  loss: 0.4310 (0.4717)  time: 10.5338  data: 0.0003  max mem: 8262\n",
            "Epoch: [10] Train  [190/330]  eta: 0:24:34  lr: 0.000053  loss: 0.6242 (0.4776)  time: 10.5328  data: 0.0002  max mem: 8262\n",
            "Epoch: [10] Train  [200/330]  eta: 0:22:48  lr: 0.000053  loss: 0.3494 (0.4726)  time: 10.5462  data: 0.0002  max mem: 8262\n",
            "Epoch: [10] Train  [210/330]  eta: 0:21:03  lr: 0.000053  loss: 0.9794 (0.4730)  time: 10.5462  data: 0.0002  max mem: 8262\n",
            "Epoch: [10] Train  [220/330]  eta: 0:19:18  lr: 0.000053  loss: 0.8349 (0.4775)  time: 10.5146  data: 0.0002  max mem: 8262\n",
            "Epoch: [10] Train  [230/330]  eta: 0:17:32  lr: 0.000053  loss: 0.3850 (0.4821)  time: 10.5089  data: 0.0002  max mem: 8262\n",
            "Epoch: [10] Train  [240/330]  eta: 0:15:47  lr: 0.000052  loss: 0.4366 (0.4798)  time: 10.5305  data: 0.0003  max mem: 8262\n",
            "Epoch: [10] Train  [250/330]  eta: 0:14:02  lr: 0.000052  loss: 0.5338 (0.4798)  time: 10.5462  data: 0.0002  max mem: 8262\n",
            "Epoch: [10] Train  [260/330]  eta: 0:12:17  lr: 0.000052  loss: 0.9836 (0.4816)  time: 10.5610  data: 0.0003  max mem: 8262\n",
            "Epoch: [10] Train  [270/330]  eta: 0:10:31  lr: 0.000052  loss: 0.4149 (0.4816)  time: 10.5361  data: 0.0003  max mem: 8262\n",
            "Epoch: [10] Train  [280/330]  eta: 0:08:46  lr: 0.000052  loss: 0.4202 (0.4785)  time: 10.4795  data: 0.0003  max mem: 8262\n",
            "Epoch: [10] Train  [290/330]  eta: 0:07:01  lr: 0.000052  loss: 1.0349 (0.4797)  time: 10.4908  data: 0.0002  max mem: 8262\n",
            "Epoch: [10] Train  [300/330]  eta: 0:05:15  lr: 0.000051  loss: 0.7474 (0.4865)  time: 10.5041  data: 0.0002  max mem: 8262\n",
            "Epoch: [10] Train  [310/330]  eta: 0:03:30  lr: 0.000051  loss: 0.2270 (0.4841)  time: 10.5036  data: 0.0003  max mem: 8262\n",
            "Epoch: [10] Train  [320/330]  eta: 0:01:45  lr: 0.000051  loss: 0.2815 (0.4875)  time: 10.5448  data: 0.0002  max mem: 8262\n",
            "Epoch: [10] Train Total time: 0:57:53\n",
            "Epoch: [10] Test  [ 0/61]  eta: 0:01:45    time: 1.7227  data: 0.6614  max mem: 8262\n",
            "Epoch: [10] Test  [10/61]  eta: 0:00:52    time: 1.0375  data: 0.0604  max mem: 8262\n",
            "Epoch: [10] Test  [20/61]  eta: 0:00:44    time: 1.0636  data: 0.0003  max mem: 8262\n",
            "Epoch: [10] Test  [30/61]  eta: 0:00:33    time: 1.1172  data: 0.0003  max mem: 8262\n",
            "Epoch: [10] Test  [40/61]  eta: 0:00:22    time: 1.0048  data: 0.0003  max mem: 8262\n",
            "Epoch: [10] Test  [50/61]  eta: 0:00:11    time: 1.0397  data: 0.0003  max mem: 8262\n",
            "Epoch: [10] Test  [60/61]  eta: 0:00:01    time: 1.0587  data: 0.0002  max mem: 8262\n",
            "Epoch: [10] Test Total time: 0:01:04\n",
            "global correct: 95.5\n",
            "average row correct: ['96.7', '98.5', '93.3', '95.7', '90.3', '94.2', '98.5', '91.5', '98.2', '66.1', '95.1', '58.9', '95.9', '97.0', '96.6', '94.8', '85.0', '96.1', '79.0', '97.6', '88.9']\n",
            "IoU: ['94.7', '88.0', '41.7', '89.9', '63.8', '75.5', '96.8', '84.4', '93.3', '48.4', '92.8', '55.8', '88.5', '92.1', '91.3', '90.7', '68.1', '84.0', '63.9', '90.6', '78.6']\n",
            "mean IoU: 79.7\n",
            "Epoch: [11] Train  [  0/330]  eta: 1:02:05  lr: 0.000051  loss: 0.3618 (0.3618)  time: 11.2907  data: 0.6173  max mem: 8262\n",
            "Epoch: [11] Train  [ 10/330]  eta: 0:56:37  lr: 0.000051  loss: 0.9173 (0.5448)  time: 10.6183  data: 0.0563  max mem: 8262\n",
            "Epoch: [11] Train  [ 20/330]  eta: 0:54:44  lr: 0.000051  loss: 1.3333 (0.6244)  time: 10.5620  data: 0.0003  max mem: 8262\n",
            "Epoch: [11] Train  [ 30/330]  eta: 0:52:45  lr: 0.000051  loss: 0.4480 (0.5962)  time: 10.5127  data: 0.0003  max mem: 8262\n",
            "Epoch: [11] Train  [ 40/330]  eta: 0:50:59  lr: 0.000050  loss: 0.8835 (0.5690)  time: 10.4978  data: 0.0004  max mem: 8262\n",
            "Epoch: [11] Train  [ 50/330]  eta: 0:49:10  lr: 0.000050  loss: 0.8250 (0.6135)  time: 10.5213  data: 0.0003  max mem: 8262\n",
            "Epoch: [11] Train  [ 60/330]  eta: 0:47:22  lr: 0.000050  loss: 0.2321 (0.5726)  time: 10.4860  data: 0.0002  max mem: 8262\n",
            "Epoch: [11] Train  [ 70/330]  eta: 0:45:38  lr: 0.000050  loss: 0.5391 (0.5607)  time: 10.5168  data: 0.0003  max mem: 8262\n",
            "Epoch: [11] Train  [ 80/330]  eta: 0:43:49  lr: 0.000050  loss: 1.3259 (0.5529)  time: 10.4810  data: 0.0003  max mem: 8262\n",
            "Epoch: [11] Train  [ 90/330]  eta: 0:42:03  lr: 0.000050  loss: 0.2249 (0.5361)  time: 10.4439  data: 0.0003  max mem: 8262\n",
            "Epoch: [11] Train  [100/330]  eta: 0:40:17  lr: 0.000049  loss: 0.3489 (0.5327)  time: 10.4839  data: 0.0003  max mem: 8262\n",
            "Epoch: [11] Train  [110/330]  eta: 0:38:31  lr: 0.000049  loss: 0.7312 (0.5291)  time: 10.4705  data: 0.0002  max mem: 8262\n",
            "Epoch: [11] Train  [120/330]  eta: 0:36:45  lr: 0.000049  loss: 0.2834 (0.5340)  time: 10.4648  data: 0.0002  max mem: 8262\n",
            "Epoch: [11] Train  [130/330]  eta: 0:34:59  lr: 0.000049  loss: 0.4069 (0.5177)  time: 10.4500  data: 0.0002  max mem: 8262\n",
            "Epoch: [11] Train  [140/330]  eta: 0:33:14  lr: 0.000049  loss: 0.2326 (0.5187)  time: 10.4606  data: 0.0003  max mem: 8262\n",
            "Epoch: [11] Train  [150/330]  eta: 0:31:29  lr: 0.000049  loss: 0.3363 (0.5090)  time: 10.4897  data: 0.0003  max mem: 8262\n",
            "Epoch: [11] Train  [160/330]  eta: 0:29:44  lr: 0.000049  loss: 0.2604 (0.5042)  time: 10.4934  data: 0.0003  max mem: 8262\n",
            "Epoch: [11] Train  [170/330]  eta: 0:27:59  lr: 0.000048  loss: 0.7962 (0.5018)  time: 10.4814  data: 0.0003  max mem: 8262\n",
            "Epoch: [11] Train  [180/330]  eta: 0:26:13  lr: 0.000048  loss: 0.8386 (0.5051)  time: 10.4644  data: 0.0003  max mem: 8262\n",
            "Epoch: [11] Train  [190/330]  eta: 0:24:28  lr: 0.000048  loss: 0.6584 (0.5062)  time: 10.4787  data: 0.0003  max mem: 8262\n",
            "Epoch: [11] Train  [200/330]  eta: 0:22:44  lr: 0.000048  loss: 0.4587 (0.5027)  time: 10.4942  data: 0.0003  max mem: 8262\n",
            "Epoch: [11] Train  [210/330]  eta: 0:20:59  lr: 0.000048  loss: 0.4542 (0.5027)  time: 10.4886  data: 0.0003  max mem: 8262\n",
            "Epoch: [11] Train  [220/330]  eta: 0:19:14  lr: 0.000048  loss: 0.9149 (0.4987)  time: 10.5251  data: 0.0003  max mem: 8262\n",
            "Epoch: [11] Train  [230/330]  eta: 0:17:29  lr: 0.000047  loss: 0.3582 (0.4975)  time: 10.5263  data: 0.0002  max mem: 8262\n",
            "Epoch: [11] Train  [240/330]  eta: 0:15:44  lr: 0.000047  loss: 0.3759 (0.5017)  time: 10.4997  data: 0.0002  max mem: 8262\n",
            "Epoch: [11] Train  [250/330]  eta: 0:13:59  lr: 0.000047  loss: 0.4188 (0.5020)  time: 10.4865  data: 0.0003  max mem: 8262\n",
            "Epoch: [11] Train  [260/330]  eta: 0:12:14  lr: 0.000047  loss: 0.4297 (0.4970)  time: 10.4758  data: 0.0003  max mem: 8262\n",
            "Epoch: [11] Train  [270/330]  eta: 0:10:29  lr: 0.000047  loss: 0.3801 (0.4969)  time: 10.5003  data: 0.0002  max mem: 8262\n",
            "Epoch: [11] Train  [280/330]  eta: 0:08:44  lr: 0.000047  loss: 0.4646 (0.4904)  time: 10.5150  data: 0.0002  max mem: 8262\n",
            "Epoch: [11] Train  [290/330]  eta: 0:06:59  lr: 0.000047  loss: 0.3139 (0.4859)  time: 10.4988  data: 0.0002  max mem: 8262\n",
            "Epoch: [11] Train  [300/330]  eta: 0:05:14  lr: 0.000046  loss: 0.7586 (0.4880)  time: 10.5390  data: 0.0002  max mem: 8262\n",
            "Epoch: [11] Train  [310/330]  eta: 0:03:29  lr: 0.000046  loss: 0.1971 (0.4897)  time: 10.5567  data: 0.0002  max mem: 8262\n",
            "Epoch: [11] Train  [320/330]  eta: 0:01:44  lr: 0.000046  loss: 0.4753 (0.4923)  time: 10.4892  data: 0.0003  max mem: 8262\n",
            "Epoch: [11] Train Total time: 0:57:44\n",
            "Epoch: [11] Test  [ 0/61]  eta: 0:01:54    time: 1.8726  data: 0.7902  max mem: 8262\n",
            "Epoch: [11] Test  [10/61]  eta: 0:00:53    time: 1.0548  data: 0.0722  max mem: 8262\n",
            "Epoch: [11] Test  [20/61]  eta: 0:00:45    time: 1.0662  data: 0.0003  max mem: 8262\n",
            "Epoch: [11] Test  [30/61]  eta: 0:00:33    time: 1.1188  data: 0.0004  max mem: 8262\n",
            "Epoch: [11] Test  [40/61]  eta: 0:00:22    time: 1.0050  data: 0.0003  max mem: 8262\n",
            "Epoch: [11] Test  [50/61]  eta: 0:00:11    time: 1.0376  data: 0.0003  max mem: 8262\n",
            "Epoch: [11] Test  [60/61]  eta: 0:00:01    time: 1.0578  data: 0.0002  max mem: 8262\n",
            "Epoch: [11] Test Total time: 0:01:04\n",
            "global correct: 95.5\n",
            "average row correct: ['96.7', '98.8', '90.7', '96.2', '90.6', '94.6', '98.8', '93.3', '97.9', '62.5', '95.9', '62.0', '96.0', '96.5', '96.5', '94.3', '84.4', '95.8', '80.2', '97.9', '89.9']\n",
            "IoU: ['94.7', '88.2', '42.7', '89.3', '64.1', '75.3', '96.7', '84.0', '93.1', '46.8', '93.2', '57.7', '89.4', '91.8', '91.5', '90.7', '67.9', '89.5', '62.8', '91.1', '74.6']\n",
            "mean IoU: 79.8\n",
            "Epoch: [12] Train  [  0/330]  eta: 1:00:58  lr: 0.000046  loss: 0.4667 (0.4667)  time: 11.0865  data: 0.5440  max mem: 8262\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "# simsiam\n",
        "!experiment_name=\"debug\";cd ../ ;\\\n",
        "    name_date=\"${experiment_name}/$(date +%Y-%m%d-%H%M%S)\";\\\n",
        "    dir=\"./results/${name_date}\";mkdir -p ${dir};dir_log=\"${dir}/output.log\";\\\n",
        "CUDA_VISIBLE_DEVICES=1,2,3,5 torchrun --nproc_per_node=4 --master_port=12345 train_multi_GPU.py \\\n",
        "    --wandb False --wandb_model dryrun --sync_bn False --amp True --aux False \\\n",
        "    --model_name aspp_contrast_resnet101 --pre_trained deeplabv3_resnet101_coco.pth \\\n",
        "    --weight_only_backbone False \\\n",
        "    --data_path pascal-voc-2012 --num_classes 21 \\\n",
        "    --epochs 20 --batch_size 8 --batch_size_val 6 --memory_size 0 \\\n",
        "    --contrast 0 --L1_loss 0.1 --L2_loss 0.1 --L3_loss 0.1\\\n",
        "    --loss_name aspp_loss --sample self_pace_epochs \\\n",
        "    --name_date $name_date \\\n",
        "    2>&1 | tee $dir_log"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
